{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanwongca/hai_work/blob/main/VTNet_att_BarChart_lit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/My Drive/msnv_data/VTNet_att/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvBjv5w1ISyD",
        "outputId": "37c6443d-181b-4f40-c5d9-96a25439dad7"
      },
      "id": "ZvBjv5w1ISyD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7de048",
      "metadata": {
        "id": "bd7de048"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/My Drive/msnv_data/VTNet_att/\")\n",
        "import utils\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE=4\n",
        "MANUAL_SEED = 1\n",
        "HIDDEN_SIZE = 256\n",
        "INPUT_SIZE=6\n",
        "utils.INPUT_SIZE = INPUT_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9aecff",
      "metadata": {
        "id": "dc9aecff"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((150,150)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94502fee",
      "metadata": {
        "id": "94502fee"
      },
      "outputs": [],
      "source": [
        "class VTNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_size=6,\n",
        "                 rnn_hidden_size=256,\n",
        "                 output_size=2,\n",
        "                 batch_size=4,\n",
        "                 rnn_type='gru',\n",
        "                 rnn_num_layers=1,\n",
        "                 n_channels_1=6,\n",
        "                 kernel_size_1=5,\n",
        "                 n_channels_2=16,\n",
        "                 kernel_size_2=5,\n",
        "                 img_n_vert=150,\n",
        "                 img_n_hor=150):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            input_size (int):\n",
        "            hidden_size (int):\n",
        "            output_size (int):\n",
        "            batch_size (int):\n",
        "            rnn_type (int):\n",
        "            num_layers (int):\n",
        "        \"\"\"\n",
        "        super(VTNet, self).__init__()\n",
        "\n",
        "        self.n_channels_2 = n_channels_2\n",
        "\n",
        "\n",
        "        # CNN portion\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_channels_1, kernel_size=kernel_size_1, stride=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=n_channels_1, out_channels=n_channels_2, kernel_size=kernel_size_2, stride=1)\n",
        "        # output size calculations\n",
        "        self.conv1_out_vert = img_n_vert - kernel_size_1 + 1\n",
        "        self.conv1_out_hor = img_n_hor - kernel_size_1 + 1\n",
        "        self.mp1_out_vert = int(np.floor((self.conv1_out_vert - 2)/2) + 1)\n",
        "        self.mp1_out_hor = int(np.floor((self.conv1_out_hor - 2) / 2) + 1)\n",
        "        self.conv2_out_vert = self.mp1_out_vert - kernel_size_2 + 1\n",
        "        self.conv2_out_hor = self.mp1_out_hor - kernel_size_2 + 1\n",
        "        self.mp2_out_vert = int(np.floor((self.conv2_out_vert - 2)/2) + 1)\n",
        "        self.mp2_out_hor = int(np.floor((self.conv2_out_hor - 2) / 2) + 1)\n",
        "        self.fc1 = nn.Linear(n_channels_2 * self.mp2_out_hor * self.mp2_out_vert, 50)\n",
        "        self.fc2 = nn.Linear(rnn_hidden_size + 50, 20)\n",
        "        self.fc3 = nn.Linear(20, output_size)\n",
        "\n",
        "        # RNN portion\n",
        "        self.multihead_attn1 = nn.MultiheadAttention(embed_dim=6, num_heads=1)\n",
        "        #self.multihead_attn2 = nn.MultiheadAttention(embed_dim=256, num_heads=1)\n",
        "        self.rnn_type = rnn_type\n",
        "        self.input_size = input_size\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.rnn_num_layers = rnn_num_layers\n",
        "\n",
        "\n",
        "\n",
        "        if rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(input_size=input_size, hidden_size=rnn_hidden_size,\n",
        "                              num_layers=rnn_num_layers)\n",
        "        elif rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(input_size=input_size, hidden_size=rnn_hidden_size,\n",
        "                               num_layers=rnn_num_layers)\n",
        "        else:\n",
        "            self.rnn = nn.RNN(input_size=input_size, hidden_size=rnn_hidden_size,\n",
        "                              num_layers=rnn_num_layers)\n",
        "\n",
        "        self.out = nn.Linear(rnn_hidden_size, output_size)\n",
        "\n",
        "    def forward(self, scan_path, time_series, hidden):\n",
        "        \"\"\"\n",
        "            Args:\n",
        "                scan_path (torch.Tensor): must be 349x231 for now\n",
        "                time_series (torch.Tensor):\n",
        "            Returns:\n",
        "                x (float): logit for confusion prediction - requires cross entropy loss\n",
        "        \"\"\"\n",
        "        x1 = self.pool(F.relu(self.conv1(scan_path)))\n",
        "        x1 = self.pool(F.relu(self.conv2(x1)))\n",
        "        x1 = x1.view(-1, self.n_channels_2 * self.mp2_out_hor * self.mp2_out_vert)\n",
        "        x1 = F.relu(self.fc1(x1))\n",
        "\n",
        "        # change input shape to (max_seq_size, batch_size, input_features):\n",
        "        x2 = time_series.permute(1, 0, 2)\n",
        "        x2, _ = self.multihead_attn1(x2, x2, x2, need_weights=False)\n",
        "        x2, hidden = self.rnn(x2, hidden)\n",
        "        #x2, _ = self.multihead_attn2(x2, x2, x2, need_weights=False)\n",
        "        x2 = x2[-1, :, :]  # take only the last output\n",
        "\n",
        "        x = torch.cat((x1, x2), 1)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\" Initializes the hidden state with zero tensors.\n",
        "        \"\"\"\n",
        "        if self.rnn_type == 'lstm':\n",
        "            return (autograd.Variable(torch.zeros(self.rnn_num_layers, batch_size, self.rnn_hidden_size)).double().to(device),\n",
        "                    autograd.Variable(torch.zeros(self.rnn_num_layers, batch_size, self.rnn_hidden_size)).double().to(device))\n",
        "        else:\n",
        "            return autograd.Variable(torch.zeros(self.rnn_num_layers, batch_size, self.rnn_hidden_size)).double().to(device)\n",
        "\n",
        "\n",
        "def st_pickle_loader(input_file_path, max_length=1000):\n",
        "    \"\"\" Processes a raw data item into a scan path image and a time series\n",
        "        for input into a STNet.\n",
        "\n",
        "        Args:\n",
        "            input_file_name (string): the name of the data item to be loaded\n",
        "            max_length (int): max number of samples to use for a given item.\n",
        "                If -1, use all samples\n",
        "        Returns:\n",
        "            item (numpy.ndarray): the fully processed data item for RNN input\n",
        "            item_sp (PIL Image):\n",
        "\n",
        "    \"\"\"\n",
        "    # Example of input_file_path = D:\\Canary\\multimodal-dl-framework\n",
        "    # \\dataset\\alzheimer\\tasks\\cookie_theft\\modalities\\preprocessed\\sequences\n",
        "    # \\eye_tracking\\augmented\\0_control\\something.pkl\n",
        "\n",
        "    transform = transforms.Compose([transforms.Resize((150,150)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))])\n",
        "\n",
        "    \"\"\"\n",
        "    0 GazePointLeftX (ADCSpx)\n",
        "    1 GazePointLeftY (ADCSpx)\n",
        "    2 GazePointRightX (ADCSpx)\n",
        "    3 GazePointRightY (ADCSpx)\n",
        "    4 GazePointX (ADCSpx)\n",
        "    5 GazePointY (ADCSpx)\n",
        "    6 GazePointX (MCSpx)\n",
        "    7 GazePointY (MCSpx)\n",
        "    8 GazePointLeftX (ADCSmm)\n",
        "    9 GazePointLeftY (ADCSmm)\n",
        "    10 GazePointRightX (ADCSmm)\n",
        "    11 GazePointRightY (ADCSmm)\n",
        "    12 DistanceLeft\n",
        "    13 DistanceRight\n",
        "    14 PupilLeft\n",
        "    15 PupilRight\n",
        "    16 FixationPointX (MCSpx)\n",
        "    17 FixationPointY (MCSpx)\n",
        "    18 FixationLength\n",
        "    \"\"\"\n",
        "\n",
        "    file = open(input_file_path, 'rb')\n",
        "    item = pickle.load(file)\n",
        "    item = item.values\n",
        "    item[:,0] = (item[:,0] + item[:,2])/2 #column 0 is now ave Gx\n",
        "    item[:,1] = (item[:,1] + item[:,3])/2 #column 1 is now ave Gy\n",
        "    item = item[item[:,0] > 0]\n",
        "    item = abs(item)\n",
        "    item[item == 1.0] = -1.0\n",
        "    item = item[:,[0, 1, 12, 13, 14, 15]] # drop all but Gx average, Gy average, Left eye distance, left eye pupil, right eye distance, right eye pupil\n",
        "\n",
        "    if len(item) == 0:\n",
        "        item = np.zeros((1000, 6))\n",
        "    else:\n",
        "        if max_length != -1:\n",
        "            item = item[-max_length:,:]\n",
        "            if len(item) < max_length:\n",
        "                num_zeros_to_pad = (max_length)-len(item)\n",
        "                item = np.append(np.zeros((num_zeros_to_pad, len(item[0]))), item, axis=0)\n",
        "    file.close()\n",
        "\n",
        "    # input_filepath_example: r\"D:\\Canary\\dataset\\augmented\\train2\\patient\\Gaze_HE-224-4.pkl\"\n",
        "    filename = input_file_path.split(os.sep)[-1].split('.')[0]\n",
        "    category = input_file_path.split(os.sep)[-3]\n",
        "\n",
        "    #check high low\n",
        "    path_to_sp = os.path.join(BASE_DIR, \"msnv_final_data\", TASK, \"high\", \"images\", filename + '.png')\n",
        "\n",
        "    # Check if the file exists, if not, try \"low\"\n",
        "    if not os.path.exists(path_to_sp):\n",
        "        path_to_sp = os.path.join(BASE_DIR, \"msnv_final_data\", TASK, \"low\", \"images\", filename + '.png')\n",
        "\n",
        "    im = Image.open(path_to_sp)\n",
        "    item_sp = transform(im)[0:3,:,:]\n",
        "    return item, item_sp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25de6ad4",
      "metadata": {
        "id": "25de6ad4"
      },
      "source": [
        "# Cookie_theft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d524cdb5",
      "metadata": {
        "id": "d524cdb5"
      },
      "outputs": [],
      "source": [
        "TASK = \"BarChartLit_label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd5795e",
      "metadata": {
        "id": "edd5795e"
      },
      "outputs": [],
      "source": [
        "def cross_validate( model_type,\n",
        "                    folds,\n",
        "                    epochs,\n",
        "                    criterion_type,\n",
        "                    optimizer_type,\n",
        "                    confused_path,\n",
        "                    not_confused_path,\n",
        "                    print_every,\n",
        "                    plot_every,\n",
        "                    hidden_size,\n",
        "                    num_layers,\n",
        "                    down_sample_training=False,\n",
        "                    learning_rate=0.0001,\n",
        "                    path_to_data_split = os.path.join(BASE_DIR, \"barchart_lit.pickle\"),\n",
        "                    verbose=False,\n",
        "                   patience=3\n",
        "                  ):\n",
        "    \"\"\"\n",
        "        Perform Cross Validation of the model using k-folds.\n",
        "\n",
        "        Args:\n",
        "            model_type (string): the type of RNN to use. Must be 'lstm', 'gru', or 'rnn'\n",
        "            epochs (int): the max number of epochs to train the model for each fold\n",
        "            criterion_type (string): the name loss function to use for training. Currently must be 'NLLLoss'\n",
        "            optimizer_type (string): the name of learning algorithm to use for training. ex 'Adam'\n",
        "            confused_path (string): the path to the folder containing the confused data samples\n",
        "            not_confused_path (string): the path to the folder containing the not_confused data samples\n",
        "            print_every (int): the number of batches to train for before printing relevant stats\n",
        "            plot_every (int): the number of batches to train for before recording relevant stats, which\n",
        "                will be plotted after each fold\n",
        "            hidden_size (int): the number of hidden units for each layer of the RNN\n",
        "            num_layers (int): the number of hidden_unit sized layers of the RNN\n",
        "            down_sample_training (boolean): if True training set will be balanced by down sampling not_confused\n",
        "            learning_rate (float): the first learning rate to be used by the optimizer\n",
        "            path_to_data_split (string): relative path to the file containing the item names for each CV fold\n",
        "            verbose (boolean): if True, function will print additional stats\n",
        "\n",
        "        Returns: (list,list,list,list,list)\n",
        "            cv_val_sens (list): list containing the validation sensitivity for each fold\n",
        "            cv_val_spec (list): list containing the validation specificity for each fold\n",
        "            cv_test_combined (list): list containing the combined test accuracy for each fold\n",
        "    \"\"\"\n",
        "\n",
        "    #ensure same items appear in folds, for reproducibility:\n",
        "    infile = open(path_to_data_split,'rb')\n",
        "    split = pickle.load(infile)\n",
        "    infile.close()\n",
        "\n",
        "    train_confused_splits = split[0]\n",
        "    test_confused_splits = split[1]\n",
        "    train_not_confused_splits = split[2]\n",
        "    test_not_confused_splits = split[3]\n",
        "\n",
        "\n",
        "    cv_test_sens = []\n",
        "    cv_test_spec = []\n",
        "    cv_test_combined = []\n",
        "    cv_auc = []\n",
        "\n",
        "    for k in range(folds):\n",
        "        print(\"\\nFold \", k+1)\n",
        "        # Get data item file names for this fold and downsample not_confused to balance training set\n",
        "        train_confused, \\\n",
        "        train_not_confused, \\\n",
        "        val_confused, \\\n",
        "        val_not_confused = \\\n",
        "        utils.get_train_val_split(train_confused_splits[k],\n",
        "                                  train_not_confused_splits[k],\n",
        "                                  percent_val_set=0.2)\n",
        "\n",
        "        if down_sample_training:\n",
        "            if len(train_not_confused) > len(train_confused):\n",
        "                train_not_confused = random.sample(train_not_confused, k=(len(train_confused)))\n",
        "            elif len(train_confused) > len(train_not_confused):\n",
        "                train_confused = random.sample(train_confused, k=(len(train_not_confused)))\n",
        "\n",
        "        test_confused = test_confused_splits[k]\n",
        "        test_not_confused = test_not_confused_splits[k]\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Patient items in training set: \", len(train_confused))\n",
        "            print(\"Control items in training set: \", len(train_not_confused))\n",
        "            print(\"Patient items in validation set: \", len(val_confused))\n",
        "            print(\"Control items in validation set: \", len(val_not_confused))\n",
        "\n",
        "        if verbose:\n",
        "            print(\"\\nTest patient items:\\n\")\n",
        "            print(test_confused)\n",
        "\n",
        "        local_train_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/train_cookie_theft/patient/')\n",
        "        local_val_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/val_cookie_theft/patient/')\n",
        "        local_test_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/test_cookie_theft/patient/')\n",
        "        local_train_not_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/train_cookie_theft/control/')\n",
        "        local_val_not_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/val_cookie_theft/control/')\n",
        "        local_test_not_confused_path = os.path.join(BASE_DIR, 'dataset/augmented/test_cookie_theft/control/')\n",
        "\n",
        "        # Remove any old directories\n",
        "        if os.path.exists(local_train_confused_path):\n",
        "            shutil.rmtree(local_train_confused_path)\n",
        "        if os.path.exists(local_val_confused_path):\n",
        "            shutil.rmtree(local_val_confused_path)\n",
        "        if os.path.exists(local_test_confused_path):\n",
        "            shutil.rmtree(local_test_confused_path)\n",
        "\n",
        "        if os.path.exists(local_train_not_confused_path):\n",
        "            shutil.rmtree(local_train_not_confused_path)\n",
        "        if os.path.exists(local_val_not_confused_path):\n",
        "            shutil.rmtree(local_val_not_confused_path)\n",
        "        if os.path.exists(local_test_not_confused_path):\n",
        "            shutil.rmtree(local_test_not_confused_path)\n",
        "\n",
        "        # Make new temp directories\n",
        "        os.makedirs(local_train_confused_path)\n",
        "        for i in train_confused:\n",
        "            shutil.copy(src=confused_path+i,dst=local_train_confused_path+i)\n",
        "\n",
        "        os.makedirs(local_val_confused_path)\n",
        "        for i in val_confused:\n",
        "            shutil.copy(src=confused_path+i,dst=local_val_confused_path+i)\n",
        "\n",
        "        os.makedirs(local_test_confused_path)\n",
        "        for i in test_confused:\n",
        "            shutil.copy(src=confused_path+i,dst=local_test_confused_path+i)\n",
        "\n",
        "        os.makedirs(local_train_not_confused_path)\n",
        "        for i in train_not_confused:\n",
        "            shutil.copy(src=not_confused_path+i,dst=local_train_not_confused_path+i)\n",
        "\n",
        "        os.makedirs(local_val_not_confused_path)\n",
        "        for i in val_not_confused:\n",
        "            shutil.copy(src=not_confused_path+i,dst=local_val_not_confused_path+i)\n",
        "\n",
        "        os.makedirs(local_test_not_confused_path)\n",
        "        for i in test_not_confused:\n",
        "            shutil.copy(src=not_confused_path+i,dst=local_test_not_confused_path+i)\n",
        "\n",
        "        # Prepare training and validation data\n",
        "        trainset = datasets.DatasetFolder(os.path.join(BASE_DIR, 'dataset/augmented/train_cookie_theft'),\n",
        "                                               loader=st_pickle_loader,\n",
        "                                               extensions='.pkl')\n",
        "\n",
        "        valset = datasets.DatasetFolder(os.path.join(BASE_DIR, 'dataset/augmented/val_cookie_theft'),\n",
        "                                                 loader=st_pickle_loader,\n",
        "                                                 extensions='.pkl')\n",
        "\n",
        "        testset = datasets.DatasetFolder(os.path.join(BASE_DIR, 'dataset/augmented/test_cookie_theft'),\n",
        "                                                 loader=st_pickle_loader,\n",
        "                                                 extensions='.pkl')\n",
        "\n",
        "\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, drop_last=True)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, drop_last=True)\n",
        "\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(valset, batch_size=4, shuffle=True, drop_last=True)\n",
        "\n",
        "        print(\"Training data: \", trainset)\n",
        "        print(\"Validation data: \", valset)\n",
        "        print(\"Test data: \", testset)\n",
        "\n",
        "        torch.manual_seed(MANUAL_SEED)\n",
        "        if model_type == 'gru':\n",
        "            model = VTNet(rnn_type='gru', rnn_num_layers=num_layers, rnn_hidden_size=hidden_size).double().to(device)\n",
        "        elif model_type == 'lstm':\n",
        "            model = VTNet(rnn_type='lstm', rnn_num_layers=num_layers, rnn_hidden_size=hidden_size).double().to(device)\n",
        "        else:\n",
        "            model = VTNet(rnn_type='rnn', rnn_num_layers=num_layers, rnn_hidden_size=hidden_size).double().to(device)\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        #save fresh model to clear any old ones out\n",
        "        torch.save(model.state_dict(), './best_STNet_fold_pd_'+str(k) +'.pt')\n",
        "        best_val_combined = 0.0\n",
        "        #Train model\n",
        "        epochs_without_improvement = 0\n",
        "        for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "            running_loss = 0.0\n",
        "            epoch_loss = 0.0\n",
        "            for i, data in enumerate(trainloader, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data\n",
        "                item, item_sp = inputs[0].double(), inputs[1].double()\n",
        "                item, item_sp, labels = item.to(device), item_sp[:,0,:,:].unsqueeze(1).to(device), labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                hidden = model.init_hidden(BATCH_SIZE)\n",
        "                # forward + backward + optimize\n",
        "                outputs = model(scan_path=item_sp, time_series=item, hidden=hidden)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                epoch_loss += loss.item()\n",
        "                if i % 10 == 0:\n",
        "                    print('[%d, %5d] loss: %.5f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "            print('epoch %d average training loss: %.5f' % (epoch + 1, epoch_loss/ len(trainloader)))\n",
        "\n",
        "            #check validation set metrics\n",
        "            running_val_loss = 0.0\n",
        "            y_true = torch.zeros((len(valloader)*4))\n",
        "            y_scores = torch.zeros((len(valloader)*4, 2))\n",
        "            with torch.no_grad():\n",
        "\n",
        "                for i, data in enumerate(valloader, 0):\n",
        "                    # get the inputs; data is a list of [inputs, labels]\n",
        "                    inputs, labels = data\n",
        "                    item, item_sp = inputs[0].double(), inputs[1].double()\n",
        "                    item, item_sp, labels = item.to(device), item_sp[:,0,:,:].unsqueeze(1).to(device), labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    hidden = model.init_hidden(BATCH_SIZE)\n",
        "                    # forward + backward + optimize\n",
        "                    outputs = model(scan_path=item_sp, time_series=item, hidden=hidden)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    running_val_loss += loss.item()\n",
        "                    #keep track of predictions\n",
        "                    num_items = outputs.shape[0]\n",
        "                    y_true[i*num_items: i*num_items + num_items] = labels\n",
        "                    y_scores[i*num_items: i*num_items + num_items, :] = outputs.squeeze()\n",
        "\n",
        "                val_loss = running_val_loss/ len(valloader)\n",
        "                print('epoch %d average val loss: %.5f' % (epoch + 1, val_loss))\n",
        "\n",
        "                #check metrics:\n",
        "                # no option to specify positive label, so flipping for confused=1\n",
        "                y_true_flipped = np.array(y_true.numpy(), copy=True)\n",
        "                y_true_flipped[y_true == 1] = 0\n",
        "                y_true_flipped[y_true == 0] = 1\n",
        "                #auc = roc_auc_score(y_true_flipped, y_scores.numpy()[:,0])\n",
        "                # roc_curve expects y_scores to be probability values of the positive class\n",
        "                fpr, tpr, thresholds = roc_curve(y_true, y_scores.numpy()[:,0], pos_label=0)\n",
        "\n",
        "                sensitivity, specificity, \\\n",
        "                accuracy = utils.optimal_threshold_sensitivity_specificity(thresholds[1:],\n",
        "                                                                           tpr[1:],\n",
        "                                                                           fpr[1:],\n",
        "                                                                           y_true,\n",
        "                                                                           y_scores.numpy()[:,0])\n",
        "                combined = (sensitivity + specificity ) / 2.0\n",
        "                print(\"epoch %d validation sens. : %.5f, spec. : %.5f; combined: %.5f\"\n",
        "                      % (epoch + 1, sensitivity, specificity, combined))\n",
        "                if combined > best_val_combined:\n",
        "                    print(\"New best validation combined accuracy found. Saving model...\")\n",
        "                    best_val_combined = combined\n",
        "                    torch.save(model.state_dict(), './best_base_STNet_fold_pd_'+str(k) +'.pt')\n",
        "                    epochs_without_improvement = 0\n",
        "                else:\n",
        "                    epochs_without_improvement = epochs_without_improvement + 1\n",
        "\n",
        "                print(\"\\n Epochs without improvement = \", epochs_without_improvement)\n",
        "\n",
        "            if epochs_without_improvement == patience:\n",
        "                print(\"\\n Stopped training because {} epochs without improvement. . .\".format(patience))\n",
        "                break\n",
        "\n",
        "        y_true = torch.tensor([]).to(device)\n",
        "        y_scores = torch.tensor([]).to(device)\n",
        "        with torch.no_grad():\n",
        "            model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n",
        "            for i, data in enumerate(testloader, 0):\n",
        "                # get the inputs; data is a list of [inputs, labels]\n",
        "                inputs, labels = data\n",
        "                item, item_sp = inputs[0].double(), inputs[1].double()\n",
        "                item, item_sp, labels = item.to(device), item_sp[:,0,:,:].unsqueeze(1).to(device), labels.to(device)\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                hidden = model.init_hidden(BATCH_SIZE)\n",
        "                # forward + backward + optimize\n",
        "                outputs = model(scan_path=item_sp, time_series=item, hidden=hidden)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_val_loss += loss.item()\n",
        "                #keep track of predictions\n",
        "                num_items = outputs.shape[0]\n",
        "                y_true = torch.cat((y_true, labels))\n",
        "                y_scores = torch.cat((y_scores,outputs.squeeze()))\n",
        "                #y_true[i*num_items: i*num_items + num_items] = labels\n",
        "                #y_scores[i*num_items: i*num_items + num_items, :] = outputs.squeeze()\n",
        "\n",
        "            #y_pred = torch.argmax(y_scores, axis=1)\n",
        "\n",
        "            #check metrics:\n",
        "            # no option to specify positive label, so flipping for confused=1\n",
        "            y_true = y_true.cpu()\n",
        "            y_scores = y_scores.cpu()\n",
        "            y_true_flipped = np.array(y_true.numpy(), copy=True)\n",
        "            y_true_flipped[y_true == 1] = 0\n",
        "            y_true_flipped[y_true == 0] = 1\n",
        "            auc = roc_auc_score(y_true_flipped, y_scores.numpy()[:,0])\n",
        "            # roc_curve expects y_scores to be probability values of the positive class\n",
        "            fpr, tpr, thresholds = roc_curve(y_true, y_scores.numpy()[:,0], pos_label=0)\n",
        "\n",
        "            sensitivity, specificity, \\\n",
        "            accuracy = utils.optimal_threshold_sensitivity_specificity(thresholds[1:],\n",
        "                                                                       tpr[1:],\n",
        "                                                                       fpr[1:],\n",
        "                                                                       y_true,\n",
        "                                                                       y_scores.numpy()[:,0])\n",
        "            combined = (sensitivity + specificity ) / 2.0\n",
        "\n",
        "            print(\"Test set sens. : %.5f, spec. : %.5f, combined: %.5f, auc: %.5f\" % (sensitivity, specificity, combined, auc))\n",
        "            cv_test_sens.append(sensitivity)\n",
        "            cv_test_spec.append(specificity)\n",
        "            cv_test_combined.append(combined)\n",
        "            cv_auc.append(auc)\n",
        "\n",
        "    print(\"\\n Average 10-fold CV test sensitivity: %.5f, specificity: %.5f, combined: %.5f, AUC: %.5f\" %\n",
        "          ((sum(cv_test_sens)/len(cv_test_sens)),\n",
        "           (sum(cv_test_spec)/len(cv_test_spec)),\n",
        "           (sum(cv_test_combined)/len(cv_test_combined)),\n",
        "           (sum(cv_auc)/len(cv_auc))))\n",
        "    return cv_test_sens, cv_test_spec, cv_test_combined, cv_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0770a7f5",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0770a7f5",
        "outputId": "9499668d-1a77-4662-8d08-5a5980d4adcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold  1\n",
            "Patient items in training set:  172\n",
            "Control items in training set:  196\n",
            "Patient items in validation set:  48\n",
            "Control items in validation set:  44\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_34_1.pkl', 'bar_34_2.pkl', 'bar_34_3.pkl', 'bar_34_4.pkl', 'bar_46_1.pkl', 'bar_46_2.pkl', 'bar_46_3.pkl', 'bar_46_4.pkl', 'ctrl_42_1.pkl', 'ctrl_42_2.pkl', 'ctrl_42_3.pkl', 'ctrl_42_4.pkl', 'ctrl_58_1.pkl', 'ctrl_58_2.pkl', 'ctrl_58_3.pkl', 'ctrl_58_4.pkl', 'ctrl_68_1.pkl', 'ctrl_68_2.pkl', 'ctrl_68_3.pkl', 'ctrl_68_4.pkl', 'ctrl_97_1.pkl', 'ctrl_97_2.pkl', 'ctrl_97_3.pkl', 'ctrl_97_4.pkl', 'link_34_1.pkl', 'link_34_2.pkl', 'link_34_3.pkl', 'link_34_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06923\n",
            "[1,    11] loss: 0.68024\n",
            "[1,    21] loss: 0.67979\n",
            "[1,    31] loss: 0.68684\n",
            "[1,    41] loss: 0.64313\n",
            "[1,    51] loss: 0.67991\n",
            "[1,    61] loss: 0.70455\n",
            "[1,    71] loss: 0.69923\n",
            "[1,    81] loss: 0.68476\n",
            "[1,    91] loss: 0.70101\n",
            "epoch 1 average training loss: 0.68455\n",
            "epoch 1 average val loss: 0.66870\n",
            "epoch 1 validation sens. : 0.88636, spec. : 0.56250; combined: 0.72443\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06631\n",
            "[2,    11] loss: 0.68890\n",
            "[2,    21] loss: 0.66126\n",
            "[2,    31] loss: 0.68232\n",
            "[2,    41] loss: 0.62948\n",
            "[2,    51] loss: 0.70029\n",
            "[2,    61] loss: 0.67531\n",
            "[2,    71] loss: 0.66308\n",
            "[2,    81] loss: 0.66325\n",
            "[2,    91] loss: 0.65846\n",
            "epoch 2 average training loss: 0.66879\n",
            "epoch 2 average val loss: 0.65587\n",
            "epoch 2 validation sens. : 0.75000, spec. : 0.62500; combined: 0.68750\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[3,     1] loss: 0.05978\n",
            "[3,    11] loss: 0.66997\n",
            "[3,    21] loss: 0.62386\n",
            "[3,    31] loss: 0.63359\n",
            "[3,    41] loss: 0.65664\n",
            "[3,    51] loss: 0.64797\n",
            "[3,    61] loss: 0.66396\n",
            "[3,    71] loss: 0.63533\n",
            "[3,    81] loss: 0.62516\n",
            "[3,    91] loss: 0.66783\n",
            "epoch 3 average training loss: 0.64652\n",
            "epoch 3 average val loss: 0.64887\n",
            "epoch 3 validation sens. : 0.70455, spec. : 0.70833; combined: 0.70644\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[4,     1] loss: 0.06001\n",
            "[4,    11] loss: 0.58768\n",
            "[4,    21] loss: 0.61019\n",
            "[4,    31] loss: 0.63802\n",
            "[4,    41] loss: 0.62600\n",
            "[4,    51] loss: 0.64058\n",
            "[4,    61] loss: 0.59499\n",
            "[4,    71] loss: 0.65105\n",
            "[4,    81] loss: 0.57334\n",
            "[4,    91] loss: 0.60177\n",
            "epoch 4 average training loss: 0.61407\n",
            "epoch 4 average val loss: 0.58964\n",
            "epoch 4 validation sens. : 0.75000, spec. : 0.66667; combined: 0.70833\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.62500, spec. : 0.67857, combined: 0.65179, auc: 0.59821\n",
            "\n",
            "Fold  2\n",
            "Patient items in training set:  176\n",
            "Control items in training set:  192\n",
            "Patient items in validation set:  48\n",
            "Control items in validation set:  44\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_35_1.pkl', 'bar_35_2.pkl', 'bar_35_3.pkl', 'bar_35_4.pkl', 'bar_49_1.pkl', 'bar_49_2.pkl', 'bar_49_3.pkl', 'bar_49_4.pkl', 'bar_77_1.pkl', 'bar_77_2.pkl', 'bar_77_3.pkl', 'bar_77_4.pkl', 'ctrl_19_1.pkl', 'ctrl_19_2.pkl', 'ctrl_19_3.pkl', 'ctrl_19_4.pkl', 'ctrl_79_1.pkl', 'ctrl_79_2.pkl', 'ctrl_79_3.pkl', 'ctrl_79_4.pkl', 'link_1_1.pkl', 'link_1_2.pkl', 'link_1_3.pkl', 'link_1_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06993\n",
            "[1,    11] loss: 0.67643\n",
            "[1,    21] loss: 0.68002\n",
            "[1,    31] loss: 0.69968\n",
            "[1,    41] loss: 0.66807\n",
            "[1,    51] loss: 0.69056\n",
            "[1,    61] loss: 0.71667\n",
            "[1,    71] loss: 0.68826\n",
            "[1,    81] loss: 0.68132\n",
            "[1,    91] loss: 0.69106\n",
            "epoch 1 average training loss: 0.68809\n",
            "epoch 1 average val loss: 0.68766\n",
            "epoch 1 validation sens. : 0.56818, spec. : 0.58333; combined: 0.57576\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.07000\n",
            "[2,    11] loss: 0.67565\n",
            "[2,    21] loss: 0.66805\n",
            "[2,    31] loss: 0.67177\n",
            "[2,    41] loss: 0.64653\n",
            "[2,    51] loss: 0.71868\n",
            "[2,    61] loss: 0.67788\n",
            "[2,    71] loss: 0.67290\n",
            "[2,    81] loss: 0.67176\n",
            "[2,    91] loss: 0.65781\n",
            "epoch 2 average training loss: 0.67293\n",
            "epoch 2 average val loss: 0.66737\n",
            "epoch 2 validation sens. : 0.56818, spec. : 0.70833; combined: 0.63826\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[3,     1] loss: 0.06687\n",
            "[3,    11] loss: 0.64058\n",
            "[3,    21] loss: 0.62846\n",
            "[3,    31] loss: 0.61185\n",
            "[3,    41] loss: 0.69750\n",
            "[3,    51] loss: 0.65193\n",
            "[3,    61] loss: 0.64378\n",
            "[3,    71] loss: 0.62922\n",
            "[3,    81] loss: 0.63568\n",
            "[3,    91] loss: 0.67167\n",
            "epoch 3 average training loss: 0.64531\n",
            "epoch 3 average val loss: 0.65298\n",
            "epoch 3 validation sens. : 0.61364, spec. : 0.62500; combined: 0.61932\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[4,     1] loss: 0.05632\n",
            "[4,    11] loss: 0.59271\n",
            "[4,    21] loss: 0.55468\n",
            "[4,    31] loss: 0.63123\n",
            "[4,    41] loss: 0.58062\n",
            "[4,    51] loss: 0.58541\n",
            "[4,    61] loss: 0.57704\n",
            "[4,    71] loss: 0.65436\n",
            "[4,    81] loss: 0.57152\n",
            "[4,    91] loss: 0.68024\n",
            "epoch 4 average training loss: 0.60330\n",
            "epoch 4 average val loss: 0.61836\n",
            "epoch 4 validation sens. : 0.75000, spec. : 0.66667; combined: 0.70833\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.05354\n",
            "[5,    11] loss: 0.53689\n",
            "[5,    21] loss: 0.55108\n",
            "[5,    31] loss: 0.57122\n",
            "[5,    41] loss: 0.59499\n",
            "[5,    51] loss: 0.49305\n",
            "[5,    61] loss: 0.54219\n",
            "[5,    71] loss: 0.45882\n",
            "[5,    81] loss: 0.57122\n",
            "[5,    91] loss: 0.51653\n",
            "epoch 5 average training loss: 0.53769\n",
            "epoch 5 average val loss: 0.58196\n",
            "epoch 5 validation sens. : 0.72727, spec. : 0.70833; combined: 0.71780\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.04771\n",
            "[6,    11] loss: 0.51323\n",
            "[6,    21] loss: 0.40701\n",
            "[6,    31] loss: 0.44925\n",
            "[6,    41] loss: 0.42011\n",
            "[6,    51] loss: 0.48389\n",
            "[6,    61] loss: 0.54174\n",
            "[6,    71] loss: 0.48351\n",
            "[6,    81] loss: 0.39469\n",
            "[6,    91] loss: 0.32141\n",
            "epoch 6 average training loss: 0.44614\n",
            "epoch 6 average val loss: 0.53471\n",
            "epoch 6 validation sens. : 0.72727, spec. : 0.75000; combined: 0.73864\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.04941\n",
            "[7,    11] loss: 0.38611\n",
            "[7,    21] loss: 0.45096\n",
            "[7,    31] loss: 0.36036\n",
            "[7,    41] loss: 0.35318\n",
            "[7,    51] loss: 0.39949\n",
            "[7,    61] loss: 0.32440\n",
            "[7,    71] loss: 0.30930\n",
            "[7,    81] loss: 0.26829\n",
            "[7,    91] loss: 0.36066\n",
            "epoch 7 average training loss: 0.35626\n",
            "epoch 7 average val loss: 0.50159\n",
            "epoch 7 validation sens. : 0.84091, spec. : 0.72917; combined: 0.78504\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[8,     1] loss: 0.02455\n",
            "[8,    11] loss: 0.31735\n",
            "[8,    21] loss: 0.32711\n",
            "[8,    31] loss: 0.26101\n",
            "[8,    41] loss: 0.23483\n",
            "[8,    51] loss: 0.38106\n",
            "[8,    61] loss: 0.38364\n",
            "[8,    71] loss: 0.25289\n",
            "[8,    81] loss: 0.22960\n",
            "[8,    91] loss: 0.27936\n",
            "epoch 8 average training loss: 0.29367\n",
            "epoch 8 average val loss: 0.48318\n",
            "epoch 8 validation sens. : 0.72727, spec. : 0.83333; combined: 0.78030\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[9,     1] loss: 0.00830\n",
            "[9,    11] loss: 0.27042\n",
            "[9,    21] loss: 0.18591\n",
            "[9,    31] loss: 0.23294\n",
            "[9,    41] loss: 0.25211\n",
            "[9,    51] loss: 0.19078\n",
            "[9,    61] loss: 0.19174\n",
            "[9,    71] loss: 0.22382\n",
            "[9,    81] loss: 0.23353\n",
            "[9,    91] loss: 0.20489\n",
            "epoch 9 average training loss: 0.21747\n",
            "epoch 9 average val loss: 0.48990\n",
            "epoch 9 validation sens. : 0.86364, spec. : 0.81250; combined: 0.83807\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[10,     1] loss: 0.01993\n",
            "[10,    11] loss: 0.10311\n",
            "[10,    21] loss: 0.14433\n",
            "[10,    31] loss: 0.16343\n",
            "[10,    41] loss: 0.14486\n",
            "[10,    51] loss: 0.18549\n",
            "[10,    61] loss: 0.12719\n",
            "[10,    71] loss: 0.15494\n",
            "[10,    81] loss: 0.15637\n",
            "[10,    91] loss: 0.10045\n",
            "epoch 10 average training loss: 0.14166\n",
            "epoch 10 average val loss: 0.48325\n",
            "epoch 10 validation sens. : 0.86364, spec. : 0.85417; combined: 0.85890\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[11,     1] loss: 0.00397\n",
            "[11,    11] loss: 0.13454\n",
            "[11,    21] loss: 0.12738\n",
            "[11,    31] loss: 0.11489\n",
            "[11,    41] loss: 0.15170\n",
            "[11,    51] loss: 0.05478\n",
            "[11,    61] loss: 0.11624\n",
            "[11,    71] loss: 0.06815\n",
            "[11,    81] loss: 0.05488\n",
            "[11,    91] loss: 0.10931\n",
            "epoch 11 average training loss: 0.10190\n",
            "epoch 11 average val loss: 0.39241\n",
            "epoch 11 validation sens. : 0.88636, spec. : 0.87500; combined: 0.88068\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[12,     1] loss: 0.00902\n",
            "[12,    11] loss: 0.07047\n",
            "[12,    21] loss: 0.08475\n",
            "[12,    31] loss: 0.05763\n",
            "[12,    41] loss: 0.07727\n",
            "[12,    51] loss: 0.06481\n",
            "[12,    61] loss: 0.07569\n",
            "[12,    71] loss: 0.06213\n",
            "[12,    81] loss: 0.09335\n",
            "[12,    91] loss: 0.07259\n",
            "epoch 12 average training loss: 0.07339\n",
            "epoch 12 average val loss: 0.38946\n",
            "epoch 12 validation sens. : 0.93182, spec. : 0.87500; combined: 0.90341\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[13,     1] loss: 0.00509\n",
            "[13,    11] loss: 0.03663\n",
            "[13,    21] loss: 0.07944\n",
            "[13,    31] loss: 0.03947\n",
            "[13,    41] loss: 0.04218\n",
            "[13,    51] loss: 0.04074\n",
            "[13,    61] loss: 0.02913\n",
            "[13,    71] loss: 0.04796\n",
            "[13,    81] loss: 0.02014\n",
            "[13,    91] loss: 0.03268\n",
            "epoch 13 average training loss: 0.04279\n",
            "epoch 13 average val loss: 0.37987\n",
            "epoch 13 validation sens. : 0.90909, spec. : 0.93750; combined: 0.92330\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[14,     1] loss: 0.00259\n",
            "[14,    11] loss: 0.03215\n",
            "[14,    21] loss: 0.03295\n",
            "[14,    31] loss: 0.03203\n",
            "[14,    41] loss: 0.04611\n",
            "[14,    51] loss: 0.02433\n",
            "[14,    61] loss: 0.03034\n",
            "[14,    71] loss: 0.03844\n",
            "[14,    81] loss: 0.05239\n",
            "[14,    91] loss: 0.04347\n",
            "epoch 14 average training loss: 0.03664\n",
            "epoch 14 average val loss: 0.38408\n",
            "epoch 14 validation sens. : 0.95455, spec. : 0.91667; combined: 0.93561\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[15,     1] loss: 0.00439\n",
            "[15,    11] loss: 0.03584\n",
            "[15,    21] loss: 0.01840\n",
            "[15,    31] loss: 0.02342\n",
            "[15,    41] loss: 0.01966\n",
            "[15,    51] loss: 0.01728\n",
            "[15,    61] loss: 0.01680\n",
            "[15,    71] loss: 0.02181\n",
            "[15,    81] loss: 0.01801\n",
            "[15,    91] loss: 0.02419\n",
            "epoch 15 average training loss: 0.02181\n",
            "epoch 15 average val loss: 0.36147\n",
            "epoch 15 validation sens. : 0.95455, spec. : 0.91667; combined: 0.93561\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[16,     1] loss: 0.00196\n",
            "[16,    11] loss: 0.02267\n",
            "[16,    21] loss: 0.01275\n",
            "[16,    31] loss: 0.01129\n",
            "[16,    41] loss: 0.01616\n",
            "[16,    51] loss: 0.01439\n",
            "[16,    61] loss: 0.01321\n",
            "[16,    71] loss: 0.01107\n",
            "[16,    81] loss: 0.01048\n",
            "[16,    91] loss: 0.01437\n",
            "epoch 16 average training loss: 0.01409\n",
            "epoch 16 average val loss: 0.36175\n",
            "epoch 16 validation sens. : 0.95455, spec. : 0.91667; combined: 0.93561\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[17,     1] loss: 0.00056\n",
            "[17,    11] loss: 0.01403\n",
            "[17,    21] loss: 0.00773\n",
            "[17,    31] loss: 0.01409\n",
            "[17,    41] loss: 0.01158\n",
            "[17,    51] loss: 0.01268\n",
            "[17,    61] loss: 0.00830\n",
            "[17,    71] loss: 0.00874\n",
            "[17,    81] loss: 0.00830\n",
            "[17,    91] loss: 0.01263\n",
            "epoch 17 average training loss: 0.01073\n",
            "epoch 17 average val loss: 0.37224\n",
            "epoch 17 validation sens. : 0.95455, spec. : 0.91667; combined: 0.93561\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.67857, spec. : 0.50000, combined: 0.58929, auc: 0.51786\n",
            "\n",
            "Fold  3\n",
            "Patient items in training set:  178\n",
            "Control items in training set:  190\n",
            "Patient items in validation set:  46\n",
            "Control items in validation set:  46\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_2_1.pkl', 'bar_2_2.pkl', 'bar_2_3.pkl', 'bar_2_4.pkl', 'bar_37_1.pkl', 'bar_37_2.pkl', 'bar_37_3.pkl', 'bar_37_4.pkl', 'bar_54_1.pkl', 'bar_54_2.pkl', 'bar_54_3.pkl', 'bar_54_4.pkl', 'ctrl_21_1.pkl', 'ctrl_21_2.pkl', 'ctrl_21_3.pkl', 'ctrl_21_4.pkl', 'ctrl_71_1.pkl', 'ctrl_71_2.pkl', 'ctrl_71_3.pkl', 'ctrl_71_4.pkl', 'link_10_1.pkl', 'link_10_2.pkl', 'link_10_3.pkl', 'link_10_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06813\n",
            "[1,    11] loss: 0.67189\n",
            "[1,    21] loss: 0.67183\n",
            "[1,    31] loss: 0.69702\n",
            "[1,    41] loss: 0.65886\n",
            "[1,    51] loss: 0.69905\n",
            "[1,    61] loss: 0.69461\n",
            "[1,    71] loss: 0.68695\n",
            "[1,    81] loss: 0.69925\n",
            "[1,    91] loss: 0.69037\n",
            "epoch 1 average training loss: 0.68505\n",
            "epoch 1 average val loss: 0.67799\n",
            "epoch 1 validation sens. : 0.50000, spec. : 0.65217; combined: 0.57609\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06991\n",
            "[2,    11] loss: 0.67254\n",
            "[2,    21] loss: 0.64229\n",
            "[2,    31] loss: 0.68040\n",
            "[2,    41] loss: 0.61916\n",
            "[2,    51] loss: 0.72500\n",
            "[2,    61] loss: 0.65175\n",
            "[2,    71] loss: 0.67972\n",
            "[2,    81] loss: 0.66231\n",
            "[2,    91] loss: 0.66551\n",
            "epoch 2 average training loss: 0.66640\n",
            "epoch 2 average val loss: 0.66310\n",
            "epoch 2 validation sens. : 0.65217, spec. : 0.56522; combined: 0.60870\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[3,     1] loss: 0.06394\n",
            "[3,    11] loss: 0.65928\n",
            "[3,    21] loss: 0.64729\n",
            "[3,    31] loss: 0.63338\n",
            "[3,    41] loss: 0.66777\n",
            "[3,    51] loss: 0.63487\n",
            "[3,    61] loss: 0.61483\n",
            "[3,    71] loss: 0.63177\n",
            "[3,    81] loss: 0.61916\n",
            "[3,    91] loss: 0.62286\n",
            "epoch 3 average training loss: 0.63662\n",
            "epoch 3 average val loss: 0.63259\n",
            "epoch 3 validation sens. : 0.60870, spec. : 0.82609; combined: 0.71739\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.05582\n",
            "[4,    11] loss: 0.57929\n",
            "[4,    21] loss: 0.59077\n",
            "[4,    31] loss: 0.61422\n",
            "[4,    41] loss: 0.58101\n",
            "[4,    51] loss: 0.58690\n",
            "[4,    61] loss: 0.56506\n",
            "[4,    71] loss: 0.57168\n",
            "[4,    81] loss: 0.61447\n",
            "[4,    91] loss: 0.61404\n",
            "epoch 4 average training loss: 0.59046\n",
            "epoch 4 average val loss: 0.60200\n",
            "epoch 4 validation sens. : 0.73913, spec. : 0.71739; combined: 0.72826\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.05891\n",
            "[5,    11] loss: 0.56697\n",
            "[5,    21] loss: 0.53238\n",
            "[5,    31] loss: 0.49664\n",
            "[5,    41] loss: 0.50091\n",
            "[5,    51] loss: 0.41232\n",
            "[5,    61] loss: 0.57178\n",
            "[5,    71] loss: 0.49075\n",
            "[5,    81] loss: 0.45516\n",
            "[5,    91] loss: 0.50296\n",
            "epoch 5 average training loss: 0.50315\n",
            "epoch 5 average val loss: 0.57505\n",
            "epoch 5 validation sens. : 0.80435, spec. : 0.78261; combined: 0.79348\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.04206\n",
            "[6,    11] loss: 0.39953\n",
            "[6,    21] loss: 0.34548\n",
            "[6,    31] loss: 0.41494\n",
            "[6,    41] loss: 0.43593\n",
            "[6,    51] loss: 0.43406\n",
            "[6,    61] loss: 0.40487\n",
            "[6,    71] loss: 0.40659\n",
            "[6,    81] loss: 0.30993\n",
            "[6,    91] loss: 0.36842\n",
            "epoch 6 average training loss: 0.39559\n",
            "epoch 6 average val loss: 0.50057\n",
            "epoch 6 validation sens. : 0.84783, spec. : 0.82609; combined: 0.83696\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.03159\n",
            "[7,    11] loss: 0.28673\n",
            "[7,    21] loss: 0.33005\n",
            "[7,    31] loss: 0.34287\n",
            "[7,    41] loss: 0.22487\n",
            "[7,    51] loss: 0.34670\n",
            "[7,    61] loss: 0.34899\n",
            "[7,    71] loss: 0.27651\n",
            "[7,    81] loss: 0.28128\n",
            "[7,    91] loss: 0.27865\n",
            "epoch 7 average training loss: 0.30184\n",
            "epoch 7 average val loss: 0.45288\n",
            "epoch 7 validation sens. : 0.82609, spec. : 0.84783; combined: 0.83696\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[8,     1] loss: 0.03808\n",
            "[8,    11] loss: 0.22320\n",
            "[8,    21] loss: 0.25848\n",
            "[8,    31] loss: 0.25064\n",
            "[8,    41] loss: 0.18022\n",
            "[8,    51] loss: 0.21347\n",
            "[8,    61] loss: 0.21819\n",
            "[8,    71] loss: 0.22436\n",
            "[8,    81] loss: 0.21215\n",
            "[8,    91] loss: 0.21972\n",
            "epoch 8 average training loss: 0.22651\n",
            "epoch 8 average val loss: 0.42175\n",
            "epoch 8 validation sens. : 0.82609, spec. : 0.80435; combined: 0.81522\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[9,     1] loss: 0.01921\n",
            "[9,    11] loss: 0.18948\n",
            "[9,    21] loss: 0.18411\n",
            "[9,    31] loss: 0.18443\n",
            "[9,    41] loss: 0.18981\n",
            "[9,    51] loss: 0.13103\n",
            "[9,    61] loss: 0.13621\n",
            "[9,    71] loss: 0.16747\n",
            "[9,    81] loss: 0.12256\n",
            "[9,    91] loss: 0.17867\n",
            "epoch 9 average training loss: 0.16624\n",
            "epoch 9 average val loss: 0.38664\n",
            "epoch 9 validation sens. : 0.82609, spec. : 0.84783; combined: 0.83696\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.57143, spec. : 0.37500, combined: 0.47321, auc: 0.34077\n",
            "\n",
            "Fold  4\n",
            "Patient items in training set:  170\n",
            "Control items in training set:  198\n",
            "Patient items in validation set:  50\n",
            "Control items in validation set:  42\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_38_1.pkl', 'bar_38_2.pkl', 'bar_38_3.pkl', 'bar_38_4.pkl', 'bar_60_1.pkl', 'bar_60_2.pkl', 'bar_60_3.pkl', 'bar_60_4.pkl', 'bar_79_1.pkl', 'bar_79_2.pkl', 'bar_79_3.pkl', 'bar_79_4.pkl', 'ctrl_25_1.pkl', 'ctrl_25_2.pkl', 'ctrl_25_3.pkl', 'ctrl_25_4.pkl', 'ctrl_61_1.pkl', 'ctrl_61_2.pkl', 'ctrl_61_3.pkl', 'ctrl_61_4.pkl', 'ctrl_81_1.pkl', 'ctrl_81_2.pkl', 'ctrl_81_3.pkl', 'ctrl_81_4.pkl', 'link_23_1.pkl', 'link_23_2.pkl', 'link_23_3.pkl', 'link_23_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06897\n",
            "[1,    11] loss: 0.66506\n",
            "[1,    21] loss: 0.66951\n",
            "[1,    31] loss: 0.68561\n",
            "[1,    41] loss: 0.64370\n",
            "[1,    51] loss: 0.66613\n",
            "[1,    61] loss: 0.72082\n",
            "[1,    71] loss: 0.70404\n",
            "[1,    81] loss: 0.67396\n",
            "[1,    91] loss: 0.70268\n",
            "epoch 1 average training loss: 0.68165\n",
            "epoch 1 average val loss: 0.67963\n",
            "epoch 1 validation sens. : 0.80952, spec. : 0.44000; combined: 0.62476\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06793\n",
            "[2,    11] loss: 0.67320\n",
            "[2,    21] loss: 0.65775\n",
            "[2,    31] loss: 0.66781\n",
            "[2,    41] loss: 0.61161\n",
            "[2,    51] loss: 0.70622\n",
            "[2,    61] loss: 0.65279\n",
            "[2,    71] loss: 0.68370\n",
            "[2,    81] loss: 0.66557\n",
            "[2,    91] loss: 0.67083\n",
            "epoch 2 average training loss: 0.66528\n",
            "epoch 2 average val loss: 0.66858\n",
            "epoch 2 validation sens. : 0.57143, spec. : 0.66000; combined: 0.61571\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[3,     1] loss: 0.06747\n",
            "[3,    11] loss: 0.65388\n",
            "[3,    21] loss: 0.63871\n",
            "[3,    31] loss: 0.64328\n",
            "[3,    41] loss: 0.67935\n",
            "[3,    51] loss: 0.65395\n",
            "[3,    61] loss: 0.66171\n",
            "[3,    71] loss: 0.63155\n",
            "[3,    81] loss: 0.62959\n",
            "[3,    91] loss: 0.62389\n",
            "epoch 3 average training loss: 0.64616\n",
            "epoch 3 average val loss: 0.66258\n",
            "epoch 3 validation sens. : 0.73810, spec. : 0.68000; combined: 0.70905\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.05792\n",
            "[4,    11] loss: 0.62594\n",
            "[4,    21] loss: 0.62168\n",
            "[4,    31] loss: 0.62704\n",
            "[4,    41] loss: 0.60095\n",
            "[4,    51] loss: 0.62735\n",
            "[4,    61] loss: 0.61833\n",
            "[4,    71] loss: 0.57448\n",
            "[4,    81] loss: 0.58357\n",
            "[4,    91] loss: 0.60153\n",
            "epoch 4 average training loss: 0.60793\n",
            "epoch 4 average val loss: 0.62393\n",
            "epoch 4 validation sens. : 0.64286, spec. : 0.64000; combined: 0.64143\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[5,     1] loss: 0.04318\n",
            "[5,    11] loss: 0.59830\n",
            "[5,    21] loss: 0.54703\n",
            "[5,    31] loss: 0.56857\n",
            "[5,    41] loss: 0.55605\n",
            "[5,    51] loss: 0.49337\n",
            "[5,    61] loss: 0.58916\n",
            "[5,    71] loss: 0.53657\n",
            "[5,    81] loss: 0.51248\n",
            "[5,    91] loss: 0.52011\n",
            "epoch 5 average training loss: 0.54736\n",
            "epoch 5 average val loss: 0.61014\n",
            "epoch 5 validation sens. : 0.57143, spec. : 0.76000; combined: 0.66571\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[6,     1] loss: 0.08116\n",
            "[6,    11] loss: 0.52351\n",
            "[6,    21] loss: 0.42414\n",
            "[6,    31] loss: 0.45035\n",
            "[6,    41] loss: 0.49599\n",
            "[6,    51] loss: 0.54127\n",
            "[6,    61] loss: 0.49452\n",
            "[6,    71] loss: 0.42184\n",
            "[6,    81] loss: 0.44466\n",
            "[6,    91] loss: 0.46625\n",
            "epoch 6 average training loss: 0.47644\n",
            "epoch 6 average val loss: 0.59185\n",
            "epoch 6 validation sens. : 0.64286, spec. : 0.68000; combined: 0.66143\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.50000, spec. : 0.82143, combined: 0.66071, auc: 0.58631\n",
            "\n",
            "Fold  5\n",
            "Patient items in training set:  171\n",
            "Control items in training set:  197\n",
            "Patient items in validation set:  53\n",
            "Control items in validation set:  39\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_39_1.pkl', 'bar_39_2.pkl', 'bar_39_3.pkl', 'bar_39_4.pkl', 'bar_63_1.pkl', 'bar_63_2.pkl', 'bar_63_3.pkl', 'bar_63_4.pkl', 'ctrl_46_1.pkl', 'ctrl_46_2.pkl', 'ctrl_46_3.pkl', 'ctrl_46_4.pkl', 'ctrl_62_1.pkl', 'ctrl_62_2.pkl', 'ctrl_62_3.pkl', 'ctrl_62_4.pkl', 'link_12_1.pkl', 'link_12_2.pkl', 'link_12_3.pkl', 'link_12_4.pkl', 'link_6_1.pkl', 'link_6_2.pkl', 'link_6_3.pkl', 'link_6_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06835\n",
            "[1,    11] loss: 0.68179\n",
            "[1,    21] loss: 0.68147\n",
            "[1,    31] loss: 0.69658\n",
            "[1,    41] loss: 0.65519\n",
            "[1,    51] loss: 0.68502\n",
            "[1,    61] loss: 0.72723\n",
            "[1,    71] loss: 0.68354\n",
            "[1,    81] loss: 0.69648\n",
            "[1,    91] loss: 0.69505\n",
            "epoch 1 average training loss: 0.69009\n",
            "epoch 1 average val loss: 0.68813\n",
            "epoch 1 validation sens. : 0.61538, spec. : 0.62264; combined: 0.61901\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06136\n",
            "[2,    11] loss: 0.67522\n",
            "[2,    21] loss: 0.65717\n",
            "[2,    31] loss: 0.65519\n",
            "[2,    41] loss: 0.61583\n",
            "[2,    51] loss: 0.72681\n",
            "[2,    61] loss: 0.67515\n",
            "[2,    71] loss: 0.67042\n",
            "[2,    81] loss: 0.65941\n",
            "[2,    91] loss: 0.65803\n",
            "epoch 2 average training loss: 0.66463\n",
            "epoch 2 average val loss: 0.65193\n",
            "epoch 2 validation sens. : 0.82051, spec. : 0.56604; combined: 0.69328\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[3,     1] loss: 0.06128\n",
            "[3,    11] loss: 0.62517\n",
            "[3,    21] loss: 0.63954\n",
            "[3,    31] loss: 0.62628\n",
            "[3,    41] loss: 0.65299\n",
            "[3,    51] loss: 0.63408\n",
            "[3,    61] loss: 0.61780\n",
            "[3,    71] loss: 0.61915\n",
            "[3,    81] loss: 0.61696\n",
            "[3,    91] loss: 0.66293\n",
            "epoch 3 average training loss: 0.63321\n",
            "epoch 3 average val loss: 0.67749\n",
            "epoch 3 validation sens. : 0.66667, spec. : 0.66038; combined: 0.66352\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[4,     1] loss: 0.06185\n",
            "[4,    11] loss: 0.55592\n",
            "[4,    21] loss: 0.55855\n",
            "[4,    31] loss: 0.58425\n",
            "[4,    41] loss: 0.60900\n",
            "[4,    51] loss: 0.58902\n",
            "[4,    61] loss: 0.57243\n",
            "[4,    71] loss: 0.60153\n",
            "[4,    81] loss: 0.60548\n",
            "[4,    91] loss: 0.65184\n",
            "epoch 4 average training loss: 0.59151\n",
            "epoch 4 average val loss: 0.59655\n",
            "epoch 4 validation sens. : 0.64103, spec. : 0.79245; combined: 0.71674\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.04364\n",
            "[5,    11] loss: 0.57063\n",
            "[5,    21] loss: 0.56688\n",
            "[5,    31] loss: 0.54816\n",
            "[5,    41] loss: 0.52720\n",
            "[5,    51] loss: 0.44120\n",
            "[5,    61] loss: 0.56702\n",
            "[5,    71] loss: 0.46069\n",
            "[5,    81] loss: 0.48711\n",
            "[5,    91] loss: 0.46088\n",
            "epoch 5 average training loss: 0.51339\n",
            "epoch 5 average val loss: 0.56242\n",
            "epoch 5 validation sens. : 0.71795, spec. : 0.71698; combined: 0.71746\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.05462\n",
            "[6,    11] loss: 0.49853\n",
            "[6,    21] loss: 0.40642\n",
            "[6,    31] loss: 0.39117\n",
            "[6,    41] loss: 0.45004\n",
            "[6,    51] loss: 0.45941\n",
            "[6,    61] loss: 0.40434\n",
            "[6,    71] loss: 0.40955\n",
            "[6,    81] loss: 0.38907\n",
            "[6,    91] loss: 0.36097\n",
            "epoch 6 average training loss: 0.42249\n",
            "epoch 6 average val loss: 0.55072\n",
            "epoch 6 validation sens. : 0.64103, spec. : 0.84906; combined: 0.74504\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.01588\n",
            "[7,    11] loss: 0.38018\n",
            "[7,    21] loss: 0.35398\n",
            "[7,    31] loss: 0.31935\n",
            "[7,    41] loss: 0.32017\n",
            "[7,    51] loss: 0.27375\n",
            "[7,    61] loss: 0.27650\n",
            "[7,    71] loss: 0.34689\n",
            "[7,    81] loss: 0.40903\n",
            "[7,    91] loss: 0.37845\n",
            "epoch 7 average training loss: 0.33589\n",
            "epoch 7 average val loss: 0.46531\n",
            "epoch 7 validation sens. : 0.87179, spec. : 0.75472; combined: 0.81326\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[8,     1] loss: 0.02696\n",
            "[8,    11] loss: 0.29232\n",
            "[8,    21] loss: 0.30538\n",
            "[8,    31] loss: 0.30754\n",
            "[8,    41] loss: 0.22467\n",
            "[8,    51] loss: 0.25738\n",
            "[8,    61] loss: 0.23516\n",
            "[8,    71] loss: 0.21508\n",
            "[8,    81] loss: 0.18829\n",
            "[8,    91] loss: 0.32771\n",
            "epoch 8 average training loss: 0.26052\n",
            "epoch 8 average val loss: 0.43150\n",
            "epoch 8 validation sens. : 0.92308, spec. : 0.77358; combined: 0.84833\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[9,     1] loss: 0.02429\n",
            "[9,    11] loss: 0.22262\n",
            "[9,    21] loss: 0.18808\n",
            "[9,    31] loss: 0.21527\n",
            "[9,    41] loss: 0.21871\n",
            "[9,    51] loss: 0.19530\n",
            "[9,    61] loss: 0.20864\n",
            "[9,    71] loss: 0.26886\n",
            "[9,    81] loss: 0.16005\n",
            "[9,    91] loss: 0.13973\n",
            "epoch 9 average training loss: 0.20301\n",
            "epoch 9 average val loss: 0.58303\n",
            "epoch 9 validation sens. : 0.87179, spec. : 0.79245; combined: 0.83212\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[10,     1] loss: 0.04250\n",
            "[10,    11] loss: 0.13319\n",
            "[10,    21] loss: 0.18818\n",
            "[10,    31] loss: 0.17101\n",
            "[10,    41] loss: 0.16101\n",
            "[10,    51] loss: 0.14840\n",
            "[10,    61] loss: 0.12043\n",
            "[10,    71] loss: 0.14062\n",
            "[10,    81] loss: 0.09718\n",
            "[10,    91] loss: 0.13959\n",
            "epoch 10 average training loss: 0.14656\n",
            "epoch 10 average val loss: 0.50584\n",
            "epoch 10 validation sens. : 0.84615, spec. : 0.86792; combined: 0.85704\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[11,     1] loss: 0.02744\n",
            "[11,    11] loss: 0.08960\n",
            "[11,    21] loss: 0.10882\n",
            "[11,    31] loss: 0.07713\n",
            "[11,    41] loss: 0.09486\n",
            "[11,    51] loss: 0.12197\n",
            "[11,    61] loss: 0.09066\n",
            "[11,    71] loss: 0.08183\n",
            "[11,    81] loss: 0.09882\n",
            "[11,    91] loss: 0.06152\n",
            "epoch 11 average training loss: 0.09872\n",
            "epoch 11 average val loss: 0.37137\n",
            "epoch 11 validation sens. : 0.87179, spec. : 0.84906; combined: 0.86043\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[12,     1] loss: 0.00627\n",
            "[12,    11] loss: 0.08719\n",
            "[12,    21] loss: 0.07061\n",
            "[12,    31] loss: 0.09470\n",
            "[12,    41] loss: 0.07406\n",
            "[12,    51] loss: 0.05895\n",
            "[12,    61] loss: 0.08852\n",
            "[12,    71] loss: 0.05191\n",
            "[12,    81] loss: 0.10321\n",
            "[12,    91] loss: 0.06788\n",
            "epoch 12 average training loss: 0.07697\n",
            "epoch 12 average val loss: 0.33415\n",
            "epoch 12 validation sens. : 0.87179, spec. : 0.92453; combined: 0.89816\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[13,     1] loss: 0.00406\n",
            "[13,    11] loss: 0.04343\n",
            "[13,    21] loss: 0.06193\n",
            "[13,    31] loss: 0.03234\n",
            "[13,    41] loss: 0.04902\n",
            "[13,    51] loss: 0.04524\n",
            "[13,    61] loss: 0.06683\n",
            "[13,    71] loss: 0.04239\n",
            "[13,    81] loss: 0.05071\n",
            "[13,    91] loss: 0.02852\n",
            "epoch 13 average training loss: 0.04687\n",
            "epoch 13 average val loss: 0.34100\n",
            "epoch 13 validation sens. : 0.89744, spec. : 0.86792; combined: 0.88268\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[14,     1] loss: 0.00380\n",
            "[14,    11] loss: 0.02741\n",
            "[14,    21] loss: 0.03640\n",
            "[14,    31] loss: 0.02976\n",
            "[14,    41] loss: 0.01890\n",
            "[14,    51] loss: 0.02831\n",
            "[14,    61] loss: 0.03107\n",
            "[14,    71] loss: 0.03783\n",
            "[14,    81] loss: 0.02258\n",
            "[14,    91] loss: 0.04914\n",
            "epoch 14 average training loss: 0.03160\n",
            "epoch 14 average val loss: 0.36274\n",
            "epoch 14 validation sens. : 0.87179, spec. : 0.90566; combined: 0.88873\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[15,     1] loss: 0.00132\n",
            "[15,    11] loss: 0.02529\n",
            "[15,    21] loss: 0.02473\n",
            "[15,    31] loss: 0.02270\n",
            "[15,    41] loss: 0.02101\n",
            "[15,    51] loss: 0.02922\n",
            "[15,    61] loss: 0.02204\n",
            "[15,    71] loss: 0.01558\n",
            "[15,    81] loss: 0.02058\n",
            "[15,    91] loss: 0.02132\n",
            "epoch 15 average training loss: 0.02249\n",
            "epoch 15 average val loss: 0.33764\n",
            "epoch 15 validation sens. : 0.87179, spec. : 0.94340; combined: 0.90760\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[16,     1] loss: 0.00021\n",
            "[16,    11] loss: 0.02063\n",
            "[16,    21] loss: 0.01661\n",
            "[16,    31] loss: 0.01380\n",
            "[16,    41] loss: 0.01727\n",
            "[16,    51] loss: 0.01635\n",
            "[16,    61] loss: 0.01808\n",
            "[16,    71] loss: 0.01386\n",
            "[16,    81] loss: 0.01511\n",
            "[16,    91] loss: 0.02058\n",
            "epoch 16 average training loss: 0.01667\n",
            "epoch 16 average val loss: 0.32583\n",
            "epoch 16 validation sens. : 0.87179, spec. : 0.94340; combined: 0.90760\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[17,     1] loss: 0.00031\n",
            "[17,    11] loss: 0.01104\n",
            "[17,    21] loss: 0.01094\n",
            "[17,    31] loss: 0.00982\n",
            "[17,    41] loss: 0.01009\n",
            "[17,    51] loss: 0.01461\n",
            "[17,    61] loss: 0.01003\n",
            "[17,    71] loss: 0.01136\n",
            "[17,    81] loss: 0.01709\n",
            "[17,    91] loss: 0.01815\n",
            "epoch 17 average training loss: 0.01243\n",
            "epoch 17 average val loss: 0.34206\n",
            "epoch 17 validation sens. : 0.87179, spec. : 0.94340; combined: 0.90760\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[18,     1] loss: 0.00178\n",
            "[18,    11] loss: 0.01373\n",
            "[18,    21] loss: 0.00903\n",
            "[18,    31] loss: 0.00982\n",
            "[18,    41] loss: 0.00928\n",
            "[18,    51] loss: 0.01133\n",
            "[18,    61] loss: 0.00952\n",
            "[18,    71] loss: 0.00977\n",
            "[18,    81] loss: 0.00606\n",
            "[18,    91] loss: 0.00683\n",
            "epoch 18 average training loss: 0.00948\n",
            "epoch 18 average val loss: 0.33771\n",
            "epoch 18 validation sens. : 0.89744, spec. : 0.90566; combined: 0.90155\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.82143, spec. : 0.25000, combined: 0.53571, auc: 0.35268\n",
            "\n",
            "Fold  6\n",
            "Patient items in training set:  178\n",
            "Control items in training set:  190\n",
            "Patient items in validation set:  46\n",
            "Control items in validation set:  46\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_10_1.pkl', 'bar_10_2.pkl', 'bar_10_3.pkl', 'bar_10_4.pkl', 'bar_22_1.pkl', 'bar_22_2.pkl', 'bar_22_3.pkl', 'bar_22_4.pkl', 'bar_66_1.pkl', 'bar_66_2.pkl', 'bar_66_3.pkl', 'bar_66_4.pkl', 'ctrl_50_1.pkl', 'ctrl_50_2.pkl', 'ctrl_50_3.pkl', 'ctrl_50_4.pkl', 'ctrl_85_1.pkl', 'ctrl_85_2.pkl', 'ctrl_85_3.pkl', 'ctrl_85_4.pkl', 'link_13_1.pkl', 'link_13_2.pkl', 'link_13_3.pkl', 'link_13_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06801\n",
            "[1,    11] loss: 0.68149\n",
            "[1,    21] loss: 0.68034\n",
            "[1,    31] loss: 0.68923\n",
            "[1,    41] loss: 0.67497\n",
            "[1,    51] loss: 0.68370\n",
            "[1,    61] loss: 0.69384\n",
            "[1,    71] loss: 0.68015\n",
            "[1,    81] loss: 0.69021\n",
            "[1,    91] loss: 0.68848\n",
            "epoch 1 average training loss: 0.68454\n",
            "epoch 1 average val loss: 0.68105\n",
            "epoch 1 validation sens. : 0.67391, spec. : 0.63043; combined: 0.65217\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06567\n",
            "[2,    11] loss: 0.68876\n",
            "[2,    21] loss: 0.65996\n",
            "[2,    31] loss: 0.68529\n",
            "[2,    41] loss: 0.63174\n",
            "[2,    51] loss: 0.70329\n",
            "[2,    61] loss: 0.64964\n",
            "[2,    71] loss: 0.65185\n",
            "[2,    81] loss: 0.65638\n",
            "[2,    91] loss: 0.66827\n",
            "epoch 2 average training loss: 0.66576\n",
            "epoch 2 average val loss: 0.66756\n",
            "epoch 2 validation sens. : 0.63043, spec. : 0.65217; combined: 0.64130\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[3,     1] loss: 0.06689\n",
            "[3,    11] loss: 0.64755\n",
            "[3,    21] loss: 0.63527\n",
            "[3,    31] loss: 0.62564\n",
            "[3,    41] loss: 0.65870\n",
            "[3,    51] loss: 0.64933\n",
            "[3,    61] loss: 0.63778\n",
            "[3,    71] loss: 0.62070\n",
            "[3,    81] loss: 0.64282\n",
            "[3,    91] loss: 0.62064\n",
            "epoch 3 average training loss: 0.63718\n",
            "epoch 3 average val loss: 0.62913\n",
            "epoch 3 validation sens. : 0.71739, spec. : 0.76087; combined: 0.73913\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.05727\n",
            "[4,    11] loss: 0.56991\n",
            "[4,    21] loss: 0.57758\n",
            "[4,    31] loss: 0.60422\n",
            "[4,    41] loss: 0.61900\n",
            "[4,    51] loss: 0.61196\n",
            "[4,    61] loss: 0.58766\n",
            "[4,    71] loss: 0.59195\n",
            "[4,    81] loss: 0.56395\n",
            "[4,    91] loss: 0.59191\n",
            "epoch 4 average training loss: 0.58965\n",
            "epoch 4 average val loss: 0.59341\n",
            "epoch 4 validation sens. : 0.89130, spec. : 0.78261; combined: 0.83696\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.04787\n",
            "[5,    11] loss: 0.54290\n",
            "[5,    21] loss: 0.52814\n",
            "[5,    31] loss: 0.55399\n",
            "[5,    41] loss: 0.47667\n",
            "[5,    51] loss: 0.48002\n",
            "[5,    61] loss: 0.53992\n",
            "[5,    71] loss: 0.48050\n",
            "[5,    81] loss: 0.47522\n",
            "[5,    91] loss: 0.48268\n",
            "epoch 5 average training loss: 0.50468\n",
            "epoch 5 average val loss: 0.55543\n",
            "epoch 5 validation sens. : 0.82609, spec. : 0.80435; combined: 0.81522\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[6,     1] loss: 0.04699\n",
            "[6,    11] loss: 0.46562\n",
            "[6,    21] loss: 0.41430\n",
            "[6,    31] loss: 0.40138\n",
            "[6,    41] loss: 0.37802\n",
            "[6,    51] loss: 0.54486\n",
            "[6,    61] loss: 0.46205\n",
            "[6,    71] loss: 0.40686\n",
            "[6,    81] loss: 0.32099\n",
            "[6,    91] loss: 0.35754\n",
            "epoch 6 average training loss: 0.41492\n",
            "epoch 6 average val loss: 0.49597\n",
            "epoch 6 validation sens. : 0.82609, spec. : 0.84783; combined: 0.83696\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[7,     1] loss: 0.02665\n",
            "[7,    11] loss: 0.37435\n",
            "[7,    21] loss: 0.42218\n",
            "[7,    31] loss: 0.29048\n",
            "[7,    41] loss: 0.27386\n",
            "[7,    51] loss: 0.26807\n",
            "[7,    61] loss: 0.34086\n",
            "[7,    71] loss: 0.31778\n",
            "[7,    81] loss: 0.30530\n",
            "[7,    91] loss: 0.31522\n",
            "epoch 7 average training loss: 0.31964\n",
            "epoch 7 average val loss: 0.46248\n",
            "epoch 7 validation sens. : 0.84783, spec. : 0.84783; combined: 0.84783\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[8,     1] loss: 0.02848\n",
            "[8,    11] loss: 0.25534\n",
            "[8,    21] loss: 0.24691\n",
            "[8,    31] loss: 0.22491\n",
            "[8,    41] loss: 0.23227\n",
            "[8,    51] loss: 0.29919\n",
            "[8,    61] loss: 0.18405\n",
            "[8,    71] loss: 0.19090\n",
            "[8,    81] loss: 0.21253\n",
            "[8,    91] loss: 0.32875\n",
            "epoch 8 average training loss: 0.24005\n",
            "epoch 8 average val loss: 0.42415\n",
            "epoch 8 validation sens. : 0.86957, spec. : 0.86957; combined: 0.86957\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[9,     1] loss: 0.03992\n",
            "[9,    11] loss: 0.20533\n",
            "[9,    21] loss: 0.16621\n",
            "[9,    31] loss: 0.22976\n",
            "[9,    41] loss: 0.19783\n",
            "[9,    51] loss: 0.16390\n",
            "[9,    61] loss: 0.17505\n",
            "[9,    71] loss: 0.21204\n",
            "[9,    81] loss: 0.12127\n",
            "[9,    91] loss: 0.18153\n",
            "epoch 9 average training loss: 0.18546\n",
            "epoch 9 average val loss: 0.42397\n",
            "epoch 9 validation sens. : 0.86957, spec. : 0.84783; combined: 0.85870\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[10,     1] loss: 0.00682\n",
            "[10,    11] loss: 0.10072\n",
            "[10,    21] loss: 0.12197\n",
            "[10,    31] loss: 0.12364\n",
            "[10,    41] loss: 0.07895\n",
            "[10,    51] loss: 0.16355\n",
            "[10,    61] loss: 0.09814\n",
            "[10,    71] loss: 0.17550\n",
            "[10,    81] loss: 0.10825\n",
            "[10,    91] loss: 0.12518\n",
            "epoch 10 average training loss: 0.12035\n",
            "epoch 10 average val loss: 0.45581\n",
            "epoch 10 validation sens. : 0.86957, spec. : 0.89130; combined: 0.88043\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[11,     1] loss: 0.01377\n",
            "[11,    11] loss: 0.10986\n",
            "[11,    21] loss: 0.07425\n",
            "[11,    31] loss: 0.09425\n",
            "[11,    41] loss: 0.10214\n",
            "[11,    51] loss: 0.07432\n",
            "[11,    61] loss: 0.08239\n",
            "[11,    71] loss: 0.04280\n",
            "[11,    81] loss: 0.04220\n",
            "[11,    91] loss: 0.06826\n",
            "epoch 11 average training loss: 0.07697\n",
            "epoch 11 average val loss: 0.37359\n",
            "epoch 11 validation sens. : 0.86957, spec. : 0.89130; combined: 0.88043\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[12,     1] loss: 0.01030\n",
            "[12,    11] loss: 0.06888\n",
            "[12,    21] loss: 0.06105\n",
            "[12,    31] loss: 0.04014\n",
            "[12,    41] loss: 0.04653\n",
            "[12,    51] loss: 0.05977\n",
            "[12,    61] loss: 0.05476\n",
            "[12,    71] loss: 0.04887\n",
            "[12,    81] loss: 0.05722\n",
            "[12,    91] loss: 0.06036\n",
            "epoch 12 average training loss: 0.05555\n",
            "epoch 12 average val loss: 0.36610\n",
            "epoch 12 validation sens. : 0.86957, spec. : 0.93478; combined: 0.90217\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[13,     1] loss: 0.00186\n",
            "[13,    11] loss: 0.03729\n",
            "[13,    21] loss: 0.05202\n",
            "[13,    31] loss: 0.02229\n",
            "[13,    41] loss: 0.03941\n",
            "[13,    51] loss: 0.03808\n",
            "[13,    61] loss: 0.02317\n",
            "[13,    71] loss: 0.03774\n",
            "[13,    81] loss: 0.03432\n",
            "[13,    91] loss: 0.04825\n",
            "epoch 13 average training loss: 0.03771\n",
            "epoch 13 average val loss: 0.41609\n",
            "epoch 13 validation sens. : 0.89130, spec. : 0.89130; combined: 0.89130\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[14,     1] loss: 0.00181\n",
            "[14,    11] loss: 0.02119\n",
            "[14,    21] loss: 0.03585\n",
            "[14,    31] loss: 0.02921\n",
            "[14,    41] loss: 0.01946\n",
            "[14,    51] loss: 0.01719\n",
            "[14,    61] loss: 0.02879\n",
            "[14,    71] loss: 0.02468\n",
            "[14,    81] loss: 0.03167\n",
            "[14,    91] loss: 0.02008\n",
            "epoch 14 average training loss: 0.02579\n",
            "epoch 14 average val loss: 0.44301\n",
            "epoch 14 validation sens. : 0.86957, spec. : 0.91304; combined: 0.89130\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[15,     1] loss: 0.00211\n",
            "[15,    11] loss: 0.01906\n",
            "[15,    21] loss: 0.02737\n",
            "[15,    31] loss: 0.00962\n",
            "[15,    41] loss: 0.01370\n",
            "[15,    51] loss: 0.01738\n",
            "[15,    61] loss: 0.01976\n",
            "[15,    71] loss: 0.02117\n",
            "[15,    81] loss: 0.00943\n",
            "[15,    91] loss: 0.01261\n",
            "epoch 15 average training loss: 0.01693\n",
            "epoch 15 average val loss: 0.40404\n",
            "epoch 15 validation sens. : 0.91304, spec. : 0.89130; combined: 0.90217\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.57143, spec. : 0.83333, combined: 0.70238, auc: 0.60417\n",
            "\n",
            "Fold  7\n",
            "Patient items in training set:  168\n",
            "Control items in training set:  200\n",
            "Patient items in validation set:  44\n",
            "Control items in validation set:  48\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_12_1.pkl', 'bar_12_2.pkl', 'bar_12_3.pkl', 'bar_12_4.pkl', 'bar_24_1.pkl', 'bar_24_2.pkl', 'bar_24_3.pkl', 'bar_24_4.pkl', 'bar_68_1.pkl', 'bar_68_2.pkl', 'bar_68_3.pkl', 'bar_68_4.pkl', 'ctrl_31_1.pkl', 'ctrl_31_2.pkl', 'ctrl_31_3.pkl', 'ctrl_31_4.pkl', 'ctrl_64_1.pkl', 'ctrl_64_2.pkl', 'ctrl_64_3.pkl', 'ctrl_64_4.pkl', 'ctrl_9_1.pkl', 'ctrl_9_2.pkl', 'ctrl_9_3.pkl', 'ctrl_9_4.pkl', 'link_15_1.pkl', 'link_15_2.pkl', 'link_15_3.pkl', 'link_15_4.pkl', 'link_19_1.pkl', 'link_19_2.pkl', 'link_19_3.pkl', 'link_19_4.pkl', 'link_3_1.pkl', 'link_3_2.pkl', 'link_3_3.pkl', 'link_3_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06832\n",
            "[1,    11] loss: 0.68289\n",
            "[1,    21] loss: 0.67953\n",
            "[1,    31] loss: 0.67904\n",
            "[1,    41] loss: 0.65183\n",
            "[1,    51] loss: 0.68706\n",
            "[1,    61] loss: 0.72449\n",
            "[1,    71] loss: 0.68763\n",
            "[1,    81] loss: 0.69420\n",
            "[1,    91] loss: 0.70761\n",
            "epoch 1 average training loss: 0.68820\n",
            "epoch 1 average val loss: 0.67986\n",
            "epoch 1 validation sens. : 0.62500, spec. : 0.70455; combined: 0.66477\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.07206\n",
            "[2,    11] loss: 0.67822\n",
            "[2,    21] loss: 0.65548\n",
            "[2,    31] loss: 0.66544\n",
            "[2,    41] loss: 0.62475\n",
            "[2,    51] loss: 0.73529\n",
            "[2,    61] loss: 0.67820\n",
            "[2,    71] loss: 0.66957\n",
            "[2,    81] loss: 0.68248\n",
            "[2,    91] loss: 0.65894\n",
            "epoch 2 average training loss: 0.67273\n",
            "epoch 2 average val loss: 0.66121\n",
            "epoch 2 validation sens. : 0.60417, spec. : 0.70455; combined: 0.65436\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[3,     1] loss: 0.06023\n",
            "[3,    11] loss: 0.65756\n",
            "[3,    21] loss: 0.65116\n",
            "[3,    31] loss: 0.62474\n",
            "[3,    41] loss: 0.69226\n",
            "[3,    51] loss: 0.61075\n",
            "[3,    61] loss: 0.65349\n",
            "[3,    71] loss: 0.64090\n",
            "[3,    81] loss: 0.62987\n",
            "[3,    91] loss: 0.66629\n",
            "epoch 3 average training loss: 0.64612\n",
            "epoch 3 average val loss: 0.63996\n",
            "epoch 3 validation sens. : 0.64583, spec. : 0.75000; combined: 0.69792\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.05944\n",
            "[4,    11] loss: 0.62548\n",
            "[4,    21] loss: 0.58923\n",
            "[4,    31] loss: 0.63555\n",
            "[4,    41] loss: 0.57999\n",
            "[4,    51] loss: 0.59124\n",
            "[4,    61] loss: 0.63727\n",
            "[4,    71] loss: 0.57882\n",
            "[4,    81] loss: 0.60516\n",
            "[4,    91] loss: 0.60660\n",
            "epoch 4 average training loss: 0.60630\n",
            "epoch 4 average val loss: 0.61035\n",
            "epoch 4 validation sens. : 0.75000, spec. : 0.70455; combined: 0.72727\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.05100\n",
            "[5,    11] loss: 0.63458\n",
            "[5,    21] loss: 0.55461\n",
            "[5,    31] loss: 0.61483\n",
            "[5,    41] loss: 0.59413\n",
            "[5,    51] loss: 0.48192\n",
            "[5,    61] loss: 0.54455\n",
            "[5,    71] loss: 0.50729\n",
            "[5,    81] loss: 0.51064\n",
            "[5,    91] loss: 0.49444\n",
            "epoch 5 average training loss: 0.54807\n",
            "epoch 5 average val loss: 0.60758\n",
            "epoch 5 validation sens. : 0.70833, spec. : 0.79545; combined: 0.75189\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.06668\n",
            "[6,    11] loss: 0.49487\n",
            "[6,    21] loss: 0.51694\n",
            "[6,    31] loss: 0.51983\n",
            "[6,    41] loss: 0.42375\n",
            "[6,    51] loss: 0.50413\n",
            "[6,    61] loss: 0.45752\n",
            "[6,    71] loss: 0.31752\n",
            "[6,    81] loss: 0.50261\n",
            "[6,    91] loss: 0.39616\n",
            "epoch 6 average training loss: 0.46328\n",
            "epoch 6 average val loss: 0.52091\n",
            "epoch 6 validation sens. : 0.75000, spec. : 0.81818; combined: 0.78409\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.02824\n",
            "[7,    11] loss: 0.43310\n",
            "[7,    21] loss: 0.37264\n",
            "[7,    31] loss: 0.36067\n",
            "[7,    41] loss: 0.33010\n",
            "[7,    51] loss: 0.30695\n",
            "[7,    61] loss: 0.29274\n",
            "[7,    71] loss: 0.43123\n",
            "[7,    81] loss: 0.34826\n",
            "[7,    91] loss: 0.36573\n",
            "epoch 7 average training loss: 0.36227\n",
            "epoch 7 average val loss: 0.45981\n",
            "epoch 7 validation sens. : 0.85417, spec. : 0.77273; combined: 0.81345\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[8,     1] loss: 0.03076\n",
            "[8,    11] loss: 0.27126\n",
            "[8,    21] loss: 0.28535\n",
            "[8,    31] loss: 0.27321\n",
            "[8,    41] loss: 0.33402\n",
            "[8,    51] loss: 0.28560\n",
            "[8,    61] loss: 0.32308\n",
            "[8,    71] loss: 0.22194\n",
            "[8,    81] loss: 0.20292\n",
            "[8,    91] loss: 0.36224\n",
            "epoch 8 average training loss: 0.28372\n",
            "epoch 8 average val loss: 0.44617\n",
            "epoch 8 validation sens. : 0.89583, spec. : 0.75000; combined: 0.82292\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[9,     1] loss: 0.02229\n",
            "[9,    11] loss: 0.24658\n",
            "[9,    21] loss: 0.16880\n",
            "[9,    31] loss: 0.23755\n",
            "[9,    41] loss: 0.16330\n",
            "[9,    51] loss: 0.20407\n",
            "[9,    61] loss: 0.23086\n",
            "[9,    71] loss: 0.19050\n",
            "[9,    81] loss: 0.23817\n",
            "[9,    91] loss: 0.13836\n",
            "epoch 9 average training loss: 0.20155\n",
            "epoch 9 average val loss: 0.54569\n",
            "epoch 9 validation sens. : 0.85417, spec. : 0.84091; combined: 0.84754\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[10,     1] loss: 0.04569\n",
            "[10,    11] loss: 0.11500\n",
            "[10,    21] loss: 0.13760\n",
            "[10,    31] loss: 0.17416\n",
            "[10,    41] loss: 0.16441\n",
            "[10,    51] loss: 0.11915\n",
            "[10,    61] loss: 0.12810\n",
            "[10,    71] loss: 0.22307\n",
            "[10,    81] loss: 0.14791\n",
            "[10,    91] loss: 0.15823\n",
            "epoch 10 average training loss: 0.15396\n",
            "epoch 10 average val loss: 0.40961\n",
            "epoch 10 validation sens. : 0.85417, spec. : 0.95455; combined: 0.90436\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[11,     1] loss: 0.00899\n",
            "[11,    11] loss: 0.14862\n",
            "[11,    21] loss: 0.07834\n",
            "[11,    31] loss: 0.10651\n",
            "[11,    41] loss: 0.16137\n",
            "[11,    51] loss: 0.13185\n",
            "[11,    61] loss: 0.11960\n",
            "[11,    71] loss: 0.07059\n",
            "[11,    81] loss: 0.09995\n",
            "[11,    91] loss: 0.08379\n",
            "epoch 11 average training loss: 0.11042\n",
            "epoch 11 average val loss: 0.34685\n",
            "epoch 11 validation sens. : 0.85417, spec. : 0.95455; combined: 0.90436\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[12,     1] loss: 0.01262\n",
            "[12,    11] loss: 0.09167\n",
            "[12,    21] loss: 0.04888\n",
            "[12,    31] loss: 0.07498\n",
            "[12,    41] loss: 0.06706\n",
            "[12,    51] loss: 0.12094\n",
            "[12,    61] loss: 0.07857\n",
            "[12,    71] loss: 0.04682\n",
            "[12,    81] loss: 0.06335\n",
            "[12,    91] loss: 0.07248\n",
            "epoch 12 average training loss: 0.07375\n",
            "epoch 12 average val loss: 0.33353\n",
            "epoch 12 validation sens. : 0.89583, spec. : 0.90909; combined: 0.90246\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[13,     1] loss: 0.00486\n",
            "[13,    11] loss: 0.04118\n",
            "[13,    21] loss: 0.03314\n",
            "[13,    31] loss: 0.04391\n",
            "[13,    41] loss: 0.06251\n",
            "[13,    51] loss: 0.03462\n",
            "[13,    61] loss: 0.04042\n",
            "[13,    71] loss: 0.03493\n",
            "[13,    81] loss: 0.05529\n",
            "[13,    91] loss: 0.05483\n",
            "epoch 13 average training loss: 0.04497\n",
            "epoch 13 average val loss: 0.35315\n",
            "epoch 13 validation sens. : 0.89583, spec. : 0.90909; combined: 0.90246\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.68750, spec. : 0.47222, combined: 0.57986, auc: 0.55903\n",
            "\n",
            "Fold  8\n",
            "Patient items in training set:  185\n",
            "Control items in training set:  183\n",
            "Patient items in validation set:  43\n",
            "Control items in validation set:  49\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_13_1.pkl', 'bar_13_2.pkl', 'bar_13_3.pkl', 'bar_13_4.pkl', 'bar_1_1.pkl', 'bar_1_2.pkl', 'bar_1_3.pkl', 'bar_1_4.pkl', 'bar_69_1.pkl', 'bar_69_2.pkl', 'bar_69_3.pkl', 'bar_69_4.pkl', 'ctrl_75_1.pkl', 'ctrl_75_2.pkl', 'ctrl_75_3.pkl', 'ctrl_75_4.pkl', 'link_16_1.pkl', 'link_16_2.pkl', 'link_16_3.pkl', 'link_16_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 368\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 92\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 52\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06885\n",
            "[1,    11] loss: 0.68386\n",
            "[1,    21] loss: 0.67782\n",
            "[1,    31] loss: 0.69700\n",
            "[1,    41] loss: 0.67901\n",
            "[1,    51] loss: 0.69002\n",
            "[1,    61] loss: 0.69414\n",
            "[1,    71] loss: 0.66678\n",
            "[1,    81] loss: 0.69202\n",
            "[1,    91] loss: 0.69010\n",
            "epoch 1 average training loss: 0.68528\n",
            "epoch 1 average val loss: 0.67903\n",
            "epoch 1 validation sens. : 0.73469, spec. : 0.60465; combined: 0.66967\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.07070\n",
            "[2,    11] loss: 0.68049\n",
            "[2,    21] loss: 0.66968\n",
            "[2,    31] loss: 0.67629\n",
            "[2,    41] loss: 0.64689\n",
            "[2,    51] loss: 0.70310\n",
            "[2,    61] loss: 0.65048\n",
            "[2,    71] loss: 0.67027\n",
            "[2,    81] loss: 0.65191\n",
            "[2,    91] loss: 0.65954\n",
            "epoch 2 average training loss: 0.66752\n",
            "epoch 2 average val loss: 0.66874\n",
            "epoch 2 validation sens. : 0.65306, spec. : 0.69767; combined: 0.67537\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[3,     1] loss: 0.07021\n",
            "[3,    11] loss: 0.64481\n",
            "[3,    21] loss: 0.66075\n",
            "[3,    31] loss: 0.63080\n",
            "[3,    41] loss: 0.66854\n",
            "[3,    51] loss: 0.64662\n",
            "[3,    61] loss: 0.61942\n",
            "[3,    71] loss: 0.59806\n",
            "[3,    81] loss: 0.62579\n",
            "[3,    91] loss: 0.61758\n",
            "epoch 3 average training loss: 0.63490\n",
            "epoch 3 average val loss: 0.63392\n",
            "epoch 3 validation sens. : 0.79592, spec. : 0.79070; combined: 0.79331\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.06023\n",
            "[4,    11] loss: 0.55114\n",
            "[4,    21] loss: 0.57035\n",
            "[4,    31] loss: 0.55253\n",
            "[4,    41] loss: 0.63001\n",
            "[4,    51] loss: 0.62428\n",
            "[4,    61] loss: 0.53565\n",
            "[4,    71] loss: 0.59322\n",
            "[4,    81] loss: 0.55508\n",
            "[4,    91] loss: 0.57112\n",
            "epoch 4 average training loss: 0.57543\n",
            "epoch 4 average val loss: 0.60162\n",
            "epoch 4 validation sens. : 0.79592, spec. : 0.81395; combined: 0.80494\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.03790\n",
            "[5,    11] loss: 0.49895\n",
            "[5,    21] loss: 0.50241\n",
            "[5,    31] loss: 0.51719\n",
            "[5,    41] loss: 0.48403\n",
            "[5,    51] loss: 0.40451\n",
            "[5,    61] loss: 0.44547\n",
            "[5,    71] loss: 0.44360\n",
            "[5,    81] loss: 0.49311\n",
            "[5,    91] loss: 0.49407\n",
            "epoch 5 average training loss: 0.47334\n",
            "epoch 5 average val loss: 0.55374\n",
            "epoch 5 validation sens. : 0.79592, spec. : 0.81395; combined: 0.80494\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[6,     1] loss: 0.03291\n",
            "[6,    11] loss: 0.40832\n",
            "[6,    21] loss: 0.36398\n",
            "[6,    31] loss: 0.37842\n",
            "[6,    41] loss: 0.31424\n",
            "[6,    51] loss: 0.41201\n",
            "[6,    61] loss: 0.41386\n",
            "[6,    71] loss: 0.31278\n",
            "[6,    81] loss: 0.29405\n",
            "[6,    91] loss: 0.33732\n",
            "epoch 6 average training loss: 0.35925\n",
            "epoch 6 average val loss: 0.47593\n",
            "epoch 6 validation sens. : 0.77551, spec. : 0.90698; combined: 0.84124\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.01983\n",
            "[7,    11] loss: 0.22880\n",
            "[7,    21] loss: 0.26474\n",
            "[7,    31] loss: 0.20917\n",
            "[7,    41] loss: 0.33756\n",
            "[7,    51] loss: 0.26475\n",
            "[7,    61] loss: 0.29325\n",
            "[7,    71] loss: 0.25908\n",
            "[7,    81] loss: 0.32535\n",
            "[7,    91] loss: 0.26171\n",
            "epoch 7 average training loss: 0.26944\n",
            "epoch 7 average val loss: 0.45017\n",
            "epoch 7 validation sens. : 0.81633, spec. : 0.83721; combined: 0.82677\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[8,     1] loss: 0.02564\n",
            "[8,    11] loss: 0.19795\n",
            "[8,    21] loss: 0.17799\n",
            "[8,    31] loss: 0.19624\n",
            "[8,    41] loss: 0.21148\n",
            "[8,    51] loss: 0.16548\n",
            "[8,    61] loss: 0.13321\n",
            "[8,    71] loss: 0.13748\n",
            "[8,    81] loss: 0.20717\n",
            "[8,    91] loss: 0.26986\n",
            "epoch 8 average training loss: 0.19050\n",
            "epoch 8 average val loss: 0.40748\n",
            "epoch 8 validation sens. : 0.79592, spec. : 0.88372; combined: 0.83982\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[9,     1] loss: 0.01850\n",
            "[9,    11] loss: 0.14108\n",
            "[9,    21] loss: 0.09341\n",
            "[9,    31] loss: 0.17545\n",
            "[9,    41] loss: 0.10160\n",
            "[9,    51] loss: 0.11603\n",
            "[9,    61] loss: 0.08360\n",
            "[9,    71] loss: 0.19102\n",
            "[9,    81] loss: 0.10806\n",
            "[9,    91] loss: 0.13188\n",
            "epoch 9 average training loss: 0.12942\n",
            "epoch 9 average val loss: 0.44176\n",
            "epoch 9 validation sens. : 0.83673, spec. : 0.90698; combined: 0.87186\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[10,     1] loss: 0.00730\n",
            "[10,    11] loss: 0.07671\n",
            "[10,    21] loss: 0.09821\n",
            "[10,    31] loss: 0.09772\n",
            "[10,    41] loss: 0.07088\n",
            "[10,    51] loss: 0.07056\n",
            "[10,    61] loss: 0.08831\n",
            "[10,    71] loss: 0.08490\n",
            "[10,    81] loss: 0.07103\n",
            "[10,    91] loss: 0.07665\n",
            "epoch 10 average training loss: 0.08088\n",
            "epoch 10 average val loss: 0.39418\n",
            "epoch 10 validation sens. : 0.83673, spec. : 0.86047; combined: 0.84860\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[11,     1] loss: 0.00383\n",
            "[11,    11] loss: 0.05004\n",
            "[11,    21] loss: 0.05294\n",
            "[11,    31] loss: 0.05799\n",
            "[11,    41] loss: 0.04599\n",
            "[11,    51] loss: 0.02939\n",
            "[11,    61] loss: 0.04416\n",
            "[11,    71] loss: 0.04641\n",
            "[11,    81] loss: 0.03603\n",
            "[11,    91] loss: 0.04236\n",
            "epoch 11 average training loss: 0.04508\n",
            "epoch 11 average val loss: 0.38409\n",
            "epoch 11 validation sens. : 0.83673, spec. : 0.86047; combined: 0.84860\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[12,     1] loss: 0.00239\n",
            "[12,    11] loss: 0.01988\n",
            "[12,    21] loss: 0.03471\n",
            "[12,    31] loss: 0.02464\n",
            "[12,    41] loss: 0.03036\n",
            "[12,    51] loss: 0.02608\n",
            "[12,    61] loss: 0.02873\n",
            "[12,    71] loss: 0.03187\n",
            "[12,    81] loss: 0.02941\n",
            "[12,    91] loss: 0.02558\n",
            "epoch 12 average training loss: 0.02778\n",
            "epoch 12 average val loss: 0.42536\n",
            "epoch 12 validation sens. : 0.85714, spec. : 0.86047; combined: 0.85880\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.81250, spec. : 0.40000, combined: 0.60625, auc: 0.47187\n",
            "\n",
            "Fold  9\n",
            "Patient items in training set:  187\n",
            "Control items in training set:  184\n",
            "Patient items in validation set:  45\n",
            "Control items in validation set:  48\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_30_1.pkl', 'bar_30_2.pkl', 'bar_30_3.pkl', 'bar_30_4.pkl', 'bar_43_1.pkl', 'bar_43_2.pkl', 'bar_43_3.pkl', 'bar_43_4.pkl', 'ctrl_12_1.pkl', 'ctrl_12_2.pkl', 'ctrl_12_3.pkl', 'ctrl_12_4.pkl', 'ctrl_76_1.pkl', 'ctrl_76_2.pkl', 'ctrl_76_3.pkl', 'ctrl_76_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 371\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 93\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 48\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.07011\n",
            "[1,    11] loss: 0.68708\n",
            "[1,    21] loss: 0.70092\n",
            "[1,    31] loss: 0.69493\n",
            "[1,    41] loss: 0.68432\n",
            "[1,    51] loss: 0.70028\n",
            "[1,    61] loss: 0.67663\n",
            "[1,    71] loss: 0.67871\n",
            "[1,    81] loss: 0.69444\n",
            "[1,    91] loss: 0.69386\n",
            "epoch 1 average training loss: 0.69039\n",
            "epoch 1 average val loss: 0.67971\n",
            "epoch 1 validation sens. : 0.57447, spec. : 0.55556; combined: 0.56501\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06682\n",
            "[2,    11] loss: 0.67754\n",
            "[2,    21] loss: 0.67978\n",
            "[2,    31] loss: 0.66310\n",
            "[2,    41] loss: 0.66970\n",
            "[2,    51] loss: 0.68451\n",
            "[2,    61] loss: 0.67194\n",
            "[2,    71] loss: 0.66967\n",
            "[2,    81] loss: 0.65419\n",
            "[2,    91] loss: 0.64834\n",
            "epoch 2 average training loss: 0.66862\n",
            "epoch 2 average val loss: 0.66261\n",
            "epoch 2 validation sens. : 0.61702, spec. : 0.77778; combined: 0.69740\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[3,     1] loss: 0.06723\n",
            "[3,    11] loss: 0.64548\n",
            "[3,    21] loss: 0.63374\n",
            "[3,    31] loss: 0.63312\n",
            "[3,    41] loss: 0.66075\n",
            "[3,    51] loss: 0.62707\n",
            "[3,    61] loss: 0.65007\n",
            "[3,    71] loss: 0.58830\n",
            "[3,    81] loss: 0.64153\n",
            "[3,    91] loss: 0.65187\n",
            "epoch 3 average training loss: 0.63770\n",
            "epoch 3 average val loss: 0.63539\n",
            "epoch 3 validation sens. : 0.64583, spec. : 0.70455; combined: 0.67519\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[4,     1] loss: 0.06466\n",
            "[4,    11] loss: 0.62091\n",
            "[4,    21] loss: 0.62709\n",
            "[4,    31] loss: 0.56792\n",
            "[4,    41] loss: 0.55684\n",
            "[4,    51] loss: 0.59818\n",
            "[4,    61] loss: 0.55255\n",
            "[4,    71] loss: 0.52601\n",
            "[4,    81] loss: 0.49800\n",
            "[4,    91] loss: 0.55669\n",
            "epoch 4 average training loss: 0.56886\n",
            "epoch 4 average val loss: 0.57374\n",
            "epoch 4 validation sens. : 0.66667, spec. : 0.84091; combined: 0.75379\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.04957\n",
            "[5,    11] loss: 0.43571\n",
            "[5,    21] loss: 0.52380\n",
            "[5,    31] loss: 0.50568\n",
            "[5,    41] loss: 0.46714\n",
            "[5,    51] loss: 0.44109\n",
            "[5,    61] loss: 0.42310\n",
            "[5,    71] loss: 0.43750\n",
            "[5,    81] loss: 0.38169\n",
            "[5,    91] loss: 0.40633\n",
            "epoch 5 average training loss: 0.45230\n",
            "epoch 5 average val loss: 0.48627\n",
            "epoch 5 validation sens. : 0.78723, spec. : 0.82222; combined: 0.80473\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.04603\n",
            "[6,    11] loss: 0.36761\n",
            "[6,    21] loss: 0.29662\n",
            "[6,    31] loss: 0.40066\n",
            "[6,    41] loss: 0.41614\n",
            "[6,    51] loss: 0.33334\n",
            "[6,    61] loss: 0.31499\n",
            "[6,    71] loss: 0.35479\n",
            "[6,    81] loss: 0.36226\n",
            "[6,    91] loss: 0.31239\n",
            "epoch 6 average training loss: 0.35003\n",
            "epoch 6 average val loss: 0.43921\n",
            "epoch 6 validation sens. : 0.89583, spec. : 0.81818; combined: 0.85701\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.02384\n",
            "[7,    11] loss: 0.22216\n",
            "[7,    21] loss: 0.23082\n",
            "[7,    31] loss: 0.26381\n",
            "[7,    41] loss: 0.29485\n",
            "[7,    51] loss: 0.30115\n",
            "[7,    61] loss: 0.23563\n",
            "[7,    71] loss: 0.21854\n",
            "[7,    81] loss: 0.30592\n",
            "[7,    91] loss: 0.22827\n",
            "epoch 7 average training loss: 0.25451\n",
            "epoch 7 average val loss: 0.44333\n",
            "epoch 7 validation sens. : 0.91489, spec. : 0.77778; combined: 0.84634\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[8,     1] loss: 0.01703\n",
            "[8,    11] loss: 0.16793\n",
            "[8,    21] loss: 0.16572\n",
            "[8,    31] loss: 0.17594\n",
            "[8,    41] loss: 0.17599\n",
            "[8,    51] loss: 0.09093\n",
            "[8,    61] loss: 0.14547\n",
            "[8,    71] loss: 0.17956\n",
            "[8,    81] loss: 0.27428\n",
            "[8,    91] loss: 0.29778\n",
            "epoch 8 average training loss: 0.18558\n",
            "epoch 8 average val loss: 0.44780\n",
            "epoch 8 validation sens. : 0.87500, spec. : 0.81818; combined: 0.84659\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[9,     1] loss: 0.02046\n",
            "[9,    11] loss: 0.30652\n",
            "[9,    21] loss: 0.13634\n",
            "[9,    31] loss: 0.11320\n",
            "[9,    41] loss: 0.11867\n",
            "[9,    51] loss: 0.08937\n",
            "[9,    61] loss: 0.07564\n",
            "[9,    71] loss: 0.22299\n",
            "[9,    81] loss: 0.19530\n",
            "[9,    91] loss: 0.09428\n",
            "epoch 9 average training loss: 0.15131\n",
            "epoch 9 average val loss: 0.35431\n",
            "epoch 9 validation sens. : 0.87234, spec. : 0.88889; combined: 0.88061\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[10,     1] loss: 0.00960\n",
            "[10,    11] loss: 0.07441\n",
            "[10,    21] loss: 0.15460\n",
            "[10,    31] loss: 0.09651\n",
            "[10,    41] loss: 0.11847\n",
            "[10,    51] loss: 0.05661\n",
            "[10,    61] loss: 0.10353\n",
            "[10,    71] loss: 0.04921\n",
            "[10,    81] loss: 0.09194\n",
            "[10,    91] loss: 0.06430\n",
            "epoch 10 average training loss: 0.09004\n",
            "epoch 10 average val loss: 0.35770\n",
            "epoch 10 validation sens. : 0.85106, spec. : 0.88889; combined: 0.86998\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[11,     1] loss: 0.00544\n",
            "[11,    11] loss: 0.06291\n",
            "[11,    21] loss: 0.05058\n",
            "[11,    31] loss: 0.06035\n",
            "[11,    41] loss: 0.08011\n",
            "[11,    51] loss: 0.05024\n",
            "[11,    61] loss: 0.07156\n",
            "[11,    71] loss: 0.04907\n",
            "[11,    81] loss: 0.09144\n",
            "[11,    91] loss: 0.08525\n",
            "epoch 11 average training loss: 0.06605\n",
            "epoch 11 average val loss: 0.34939\n",
            "epoch 11 validation sens. : 0.89362, spec. : 0.86667; combined: 0.88014\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[12,     1] loss: 0.00193\n",
            "[12,    11] loss: 0.06238\n",
            "[12,    21] loss: 0.04863\n",
            "[12,    31] loss: 0.04245\n",
            "[12,    41] loss: 0.03688\n",
            "[12,    51] loss: 0.05233\n",
            "[12,    61] loss: 0.04881\n",
            "[12,    71] loss: 0.03619\n",
            "[12,    81] loss: 0.02702\n",
            "[12,    91] loss: 0.06841\n",
            "epoch 12 average training loss: 0.04644\n",
            "epoch 12 average val loss: 0.31432\n",
            "epoch 12 validation sens. : 0.89362, spec. : 0.88889; combined: 0.89125\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[13,     1] loss: 0.00141\n",
            "[13,    11] loss: 0.02709\n",
            "[13,    21] loss: 0.03114\n",
            "[13,    31] loss: 0.02123\n",
            "[13,    41] loss: 0.02924\n",
            "[13,    51] loss: 0.02473\n",
            "[13,    61] loss: 0.02159\n",
            "[13,    71] loss: 0.04596\n",
            "[13,    81] loss: 0.03609\n",
            "[13,    91] loss: 0.01010\n",
            "epoch 13 average training loss: 0.02708\n",
            "epoch 13 average val loss: 0.31853\n",
            "epoch 13 validation sens. : 0.87500, spec. : 0.88636; combined: 0.88068\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[14,     1] loss: 0.00110\n",
            "[14,    11] loss: 0.02316\n",
            "[14,    21] loss: 0.00980\n",
            "[14,    31] loss: 0.02024\n",
            "[14,    41] loss: 0.02361\n",
            "[14,    51] loss: 0.01623\n",
            "[14,    61] loss: 0.02324\n",
            "[14,    71] loss: 0.02467\n",
            "[14,    81] loss: 0.02947\n",
            "[14,    91] loss: 0.02407\n",
            "epoch 14 average training loss: 0.02141\n",
            "epoch 14 average val loss: 0.30520\n",
            "epoch 14 validation sens. : 0.89362, spec. : 0.88889; combined: 0.89125\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[15,     1] loss: 0.00024\n",
            "[15,    11] loss: 0.01505\n",
            "[15,    21] loss: 0.01742\n",
            "[15,    31] loss: 0.01484\n",
            "[15,    41] loss: 0.01113\n",
            "[15,    51] loss: 0.01292\n",
            "[15,    61] loss: 0.01753\n",
            "[15,    71] loss: 0.01649\n",
            "[15,    81] loss: 0.01312\n",
            "[15,    91] loss: 0.01296\n",
            "epoch 15 average training loss: 0.01441\n",
            "epoch 15 average val loss: 0.31370\n",
            "epoch 15 validation sens. : 0.89583, spec. : 0.90909; combined: 0.90246\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[16,     1] loss: 0.00016\n",
            "[16,    11] loss: 0.00759\n",
            "[16,    21] loss: 0.01393\n",
            "[16,    31] loss: 0.01268\n",
            "[16,    41] loss: 0.01348\n",
            "[16,    51] loss: 0.01694\n",
            "[16,    61] loss: 0.00968\n",
            "[16,    71] loss: 0.01149\n",
            "[16,    81] loss: 0.00895\n",
            "[16,    91] loss: 0.00864\n",
            "epoch 16 average training loss: 0.01128\n",
            "epoch 16 average val loss: 0.31942\n",
            "epoch 16 validation sens. : 0.89583, spec. : 0.88636; combined: 0.89110\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[17,     1] loss: 0.00091\n",
            "[17,    11] loss: 0.00997\n",
            "[17,    21] loss: 0.00861\n",
            "[17,    31] loss: 0.00654\n",
            "[17,    41] loss: 0.00715\n",
            "[17,    51] loss: 0.00929\n",
            "[17,    61] loss: 0.00798\n",
            "[17,    71] loss: 0.01027\n",
            "[17,    81] loss: 0.01265\n",
            "[17,    91] loss: 0.00489\n",
            "epoch 17 average training loss: 0.00864\n",
            "epoch 17 average val loss: 0.32368\n",
            "epoch 17 validation sens. : 0.89583, spec. : 0.88636; combined: 0.89110\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[18,     1] loss: 0.00107\n",
            "[18,    11] loss: 0.00693\n",
            "[18,    21] loss: 0.00619\n",
            "[18,    31] loss: 0.00928\n",
            "[18,    41] loss: 0.00595\n",
            "[18,    51] loss: 0.00649\n",
            "[18,    61] loss: 0.00444\n",
            "[18,    71] loss: 0.00777\n",
            "[18,    81] loss: 0.00880\n",
            "[18,    91] loss: 0.00508\n",
            "epoch 18 average training loss: 0.00675\n",
            "epoch 18 average val loss: 0.33357\n",
            "epoch 18 validation sens. : 0.89583, spec. : 0.88636; combined: 0.89110\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.25000, spec. : 0.87500, combined: 0.56250, auc: 0.32227\n",
            "\n",
            "Fold  10\n",
            "Patient items in training set:  183\n",
            "Control items in training set:  188\n",
            "Patient items in validation set:  41\n",
            "Control items in validation set:  52\n",
            "\n",
            "Test patient items:\n",
            "\n",
            "['bar_44_1.pkl', 'bar_44_2.pkl', 'bar_44_3.pkl', 'bar_44_4.pkl', 'ctrl_40_1.pkl', 'ctrl_40_2.pkl', 'ctrl_40_3.pkl', 'ctrl_40_4.pkl', 'ctrl_55_1.pkl', 'ctrl_55_2.pkl', 'ctrl_55_3.pkl', 'ctrl_55_4.pkl', 'ctrl_67_1.pkl', 'ctrl_67_2.pkl', 'ctrl_67_3.pkl', 'ctrl_67_4.pkl', 'ctrl_77_1.pkl', 'ctrl_77_2.pkl', 'ctrl_77_3.pkl', 'ctrl_77_4.pkl', 'link_33_1.pkl', 'link_33_2.pkl', 'link_33_3.pkl', 'link_33_4.pkl']\n",
            "Training data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 371\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/train_cookie_theft\n",
            "Validation data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 93\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/val_cookie_theft\n",
            "Test data:  Dataset DatasetFolder\n",
            "    Number of datapoints: 48\n",
            "    Root location: /content/drive/My Drive/msnv_data/VTNet_att/dataset/augmented/test_cookie_theft\n",
            "[1,     1] loss: 0.06997\n",
            "[1,    11] loss: 0.68181\n",
            "[1,    21] loss: 0.69979\n",
            "[1,    31] loss: 0.69597\n",
            "[1,    41] loss: 0.67966\n",
            "[1,    51] loss: 0.70883\n",
            "[1,    61] loss: 0.68286\n",
            "[1,    71] loss: 0.68433\n",
            "[1,    81] loss: 0.69690\n",
            "[1,    91] loss: 0.67966\n",
            "epoch 1 average training loss: 0.68979\n",
            "epoch 1 average val loss: 0.68570\n",
            "epoch 1 validation sens. : 0.70588, spec. : 0.53659; combined: 0.62123\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[2,     1] loss: 0.06953\n",
            "[2,    11] loss: 0.67355\n",
            "[2,    21] loss: 0.66890\n",
            "[2,    31] loss: 0.66005\n",
            "[2,    41] loss: 0.65912\n",
            "[2,    51] loss: 0.67992\n",
            "[2,    61] loss: 0.67818\n",
            "[2,    71] loss: 0.69839\n",
            "[2,    81] loss: 0.63824\n",
            "[2,    91] loss: 0.65301\n",
            "epoch 2 average training loss: 0.66770\n",
            "epoch 2 average val loss: 0.67569\n",
            "epoch 2 validation sens. : 0.54902, spec. : 0.68293; combined: 0.61597\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[3,     1] loss: 0.06454\n",
            "[3,    11] loss: 0.63767\n",
            "[3,    21] loss: 0.64611\n",
            "[3,    31] loss: 0.64393\n",
            "[3,    41] loss: 0.67074\n",
            "[3,    51] loss: 0.65568\n",
            "[3,    61] loss: 0.65051\n",
            "[3,    71] loss: 0.58840\n",
            "[3,    81] loss: 0.67364\n",
            "[3,    91] loss: 0.62918\n",
            "epoch 3 average training loss: 0.64512\n",
            "epoch 3 average val loss: 0.66008\n",
            "epoch 3 validation sens. : 0.63462, spec. : 0.67500; combined: 0.65481\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[4,     1] loss: 0.06034\n",
            "[4,    11] loss: 0.61792\n",
            "[4,    21] loss: 0.62398\n",
            "[4,    31] loss: 0.56714\n",
            "[4,    41] loss: 0.58531\n",
            "[4,    51] loss: 0.60537\n",
            "[4,    61] loss: 0.54934\n",
            "[4,    71] loss: 0.60142\n",
            "[4,    81] loss: 0.56120\n",
            "[4,    91] loss: 0.54693\n",
            "epoch 4 average training loss: 0.58344\n",
            "epoch 4 average val loss: 0.61996\n",
            "epoch 4 validation sens. : 0.71154, spec. : 0.70000; combined: 0.70577\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[5,     1] loss: 0.05220\n",
            "[5,    11] loss: 0.47312\n",
            "[5,    21] loss: 0.52753\n",
            "[5,    31] loss: 0.53983\n",
            "[5,    41] loss: 0.48132\n",
            "[5,    51] loss: 0.46468\n",
            "[5,    61] loss: 0.48283\n",
            "[5,    71] loss: 0.39350\n",
            "[5,    81] loss: 0.46656\n",
            "[5,    91] loss: 0.49024\n",
            "epoch 5 average training loss: 0.48043\n",
            "epoch 5 average val loss: 0.58557\n",
            "epoch 5 validation sens. : 0.72549, spec. : 0.75610; combined: 0.74079\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[6,     1] loss: 0.04724\n",
            "[6,    11] loss: 0.37862\n",
            "[6,    21] loss: 0.43559\n",
            "[6,    31] loss: 0.48290\n",
            "[6,    41] loss: 0.37716\n",
            "[6,    51] loss: 0.32313\n",
            "[6,    61] loss: 0.37268\n",
            "[6,    71] loss: 0.34754\n",
            "[6,    81] loss: 0.32876\n",
            "[6,    91] loss: 0.42180\n",
            "epoch 6 average training loss: 0.38555\n",
            "epoch 6 average val loss: 0.52777\n",
            "epoch 6 validation sens. : 0.78846, spec. : 0.77500; combined: 0.78173\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[7,     1] loss: 0.02210\n",
            "[7,    11] loss: 0.26826\n",
            "[7,    21] loss: 0.21940\n",
            "[7,    31] loss: 0.25787\n",
            "[7,    41] loss: 0.35419\n",
            "[7,    51] loss: 0.27686\n",
            "[7,    61] loss: 0.39962\n",
            "[7,    71] loss: 0.25892\n",
            "[7,    81] loss: 0.29160\n",
            "[7,    91] loss: 0.19327\n",
            "epoch 7 average training loss: 0.28060\n",
            "epoch 7 average val loss: 0.47289\n",
            "epoch 7 validation sens. : 0.80392, spec. : 0.80488; combined: 0.80440\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[8,     1] loss: 0.01325\n",
            "[8,    11] loss: 0.22180\n",
            "[8,    21] loss: 0.26855\n",
            "[8,    31] loss: 0.18433\n",
            "[8,    41] loss: 0.17007\n",
            "[8,    51] loss: 0.17601\n",
            "[8,    61] loss: 0.18979\n",
            "[8,    71] loss: 0.15051\n",
            "[8,    81] loss: 0.16107\n",
            "[8,    91] loss: 0.40623\n",
            "epoch 8 average training loss: 0.21167\n",
            "epoch 8 average val loss: 0.44086\n",
            "epoch 8 validation sens. : 0.90385, spec. : 0.82500; combined: 0.86442\n",
            "New best validation combined accuracy found. Saving model...\n",
            "\n",
            " Epochs without improvement =  0\n",
            "[9,     1] loss: 0.01295\n",
            "[9,    11] loss: 0.14435\n",
            "[9,    21] loss: 0.14329\n",
            "[9,    31] loss: 0.13150\n",
            "[9,    41] loss: 0.11994\n",
            "[9,    51] loss: 0.21685\n",
            "[9,    61] loss: 0.19437\n",
            "[9,    71] loss: 0.13615\n",
            "[9,    81] loss: 0.13690\n",
            "[9,    91] loss: 0.18425\n",
            "epoch 9 average training loss: 0.15511\n",
            "epoch 9 average val loss: 0.42499\n",
            "epoch 9 validation sens. : 0.92157, spec. : 0.78049; combined: 0.85103\n",
            "\n",
            " Epochs without improvement =  1\n",
            "[10,     1] loss: 0.01825\n",
            "[10,    11] loss: 0.11998\n",
            "[10,    21] loss: 0.15151\n",
            "[10,    31] loss: 0.11569\n",
            "[10,    41] loss: 0.10989\n",
            "[10,    51] loss: 0.10751\n",
            "[10,    61] loss: 0.14165\n",
            "[10,    71] loss: 0.07573\n",
            "[10,    81] loss: 0.09668\n",
            "[10,    91] loss: 0.06397\n",
            "epoch 10 average training loss: 0.11040\n",
            "epoch 10 average val loss: 0.40842\n",
            "epoch 10 validation sens. : 0.84314, spec. : 0.85366; combined: 0.84840\n",
            "\n",
            " Epochs without improvement =  2\n",
            "[11,     1] loss: 0.01097\n",
            "[11,    11] loss: 0.14239\n",
            "[11,    21] loss: 0.10040\n",
            "[11,    31] loss: 0.07042\n",
            "[11,    41] loss: 0.04148\n",
            "[11,    51] loss: 0.07483\n",
            "[11,    61] loss: 0.10462\n",
            "[11,    71] loss: 0.03733\n",
            "[11,    81] loss: 0.07446\n",
            "[11,    91] loss: 0.02680\n",
            "epoch 11 average training loss: 0.07486\n",
            "epoch 11 average val loss: 0.41783\n",
            "epoch 11 validation sens. : 0.92157, spec. : 0.80488; combined: 0.86322\n",
            "\n",
            " Epochs without improvement =  3\n",
            "\n",
            " Stopped training because 3 epochs without improvement. . .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2101257024e0>:267: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('./best_base_STNet_fold_pd_'+str(k) +'.pt', map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set sens. : 0.41667, spec. : 0.37500, combined: 0.39583, auc: 0.28646\n",
            "\n",
            " Average 10-fold CV test sensitivity: 0.59345, specificity: 0.55806, combined: 0.57575, AUC: 0.46396\n",
            "sensitivities:  [0.593452380952381]\n",
            "specificities:  [0.5580555555555555]\n",
            "combined:  [0.5757539682539682]\n",
            "auc:  [0.46396267361111104]\n",
            "average sensitivity:  0.593452380952381\n",
            "average specificity:  0.5580555555555555\n",
            "average combined:  0.5757539682539682\n",
            "average auc:  0.46396267361111104\n"
          ]
        }
      ],
      "source": [
        "# compute for 10 different seeds\n",
        "sens = []\n",
        "spec = []\n",
        "comb = []\n",
        "auc = []\n",
        "\n",
        "\n",
        "for i in range(1):\n",
        "# baseline 10-fold CV with GRU\n",
        "    np.random.seed(MANUAL_SEED+i)\n",
        "    random.seed(MANUAL_SEED+i)\n",
        "    torch.manual_seed(MANUAL_SEED+i)\n",
        "\n",
        "    sens_list, spec_list, comb_list, auc_list = cross_validate(model_type='gru',\n",
        "                                               folds=10,\n",
        "                                               epochs=100,\n",
        "                                               criterion_type='NLLLoss',\n",
        "                                               optimizer_type='Adam',\n",
        "                                               confused_path=os.path.join(\n",
        "    BASE_DIR, \"msnv_final_data\",\n",
        "    TASK, \"high/pickle_files/\"\n",
        "),\n",
        "                                               not_confused_path=os.path.join(\n",
        "    BASE_DIR, \"msnv_final_data\",\n",
        "    TASK, \"low/pickle_files/\"\n",
        "),\n",
        "                                               print_every=1,\n",
        "                                               plot_every=1,\n",
        "                                               hidden_size=HIDDEN_SIZE,\n",
        "                                               down_sample_training=False,\n",
        "                                               num_layers=1,\n",
        "                                               learning_rate=0.0001,\n",
        "                                               verbose=True)\n",
        "    # add mean of each measure for 10-fold CV to list\n",
        "    sens.append(np.mean(sens_list))\n",
        "    spec.append(np.mean(spec_list))\n",
        "    comb.append(np.mean(comb_list))\n",
        "    auc.append(np.mean(auc_list))\n",
        "\n",
        "print(\"sensitivities: \", sens)\n",
        "print(\"specificities: \", spec)\n",
        "print(\"combined: \", comb)\n",
        "print(\"auc: \", auc)\n",
        "\n",
        "print(\"average sensitivity: \", np.mean(sens))\n",
        "print(\"average specificity: \", np.mean(spec))\n",
        "print(\"average combined: \", np.mean(comb))\n",
        "print(\"average auc: \", np.mean(auc))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}