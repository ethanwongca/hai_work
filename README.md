# Human-AI Interaction: Eye-Tracking Analysis with VTNet 

This repository contains code and documentation from research on Human-AI Interaction, focusing on eye-tracking data analysis using VTNet. T

### Key Features

- **Eye-tracking Data Processing**: Transform raw Tobii data (120 Hz) into VTNet-compatible formats
- **Cross-validation Frameworks**: Specialized CV approaches for different classification scenarios
- **HPC Integration**: Ready-to-use scripts for high-performance computing environments
- **Documentation**: Documents what files to look for in the lab server, how to use VTNet, HPCs, and more

## ğŸ“ Repository Structure

### [`Dataset`](./Dataset/)
Contains documentation on locating the necessary data files within the lab repository.

### [`preprocess`](./preprocess/) 
Tools for transforming raw eye-tracking data:
- ğŸ§¹ **Data Cleaning & Formatting**: Scripts to clean and standardize raw Tobii data
- ğŸ“Š **Exploratory Data Analysis**: Functions for visualizing and understanding eye-tracking patterns
- ğŸ—ï¸ **Dataset Construction**: Tools for building within-task and across-task datasets
- ğŸ› ï¸ **Utility Functions**: Including scanpath extraction and cyclic data splitting

### [`hpc`](./hpc/)
Resources for high-performance computing:
- ğŸ“ **Slurm Templates**: Ready-to-use templates for the `ubc_ml` cluster
- ğŸ“ˆ **Statistical Analysis**: Scripts for computing performance metrics from VTNet outputs

### [`VTNet_within_tasks`](./VTNet_within_tasks/)
Implementations of VTNet for the within-task dataset:
- â±ï¸ Multiple sequence length configurations (14s, 29s, and length 1000)
- ğŸ“„ Documentation on required VTNet modifications
- âš ï¸ **Important**: The `st_pickle_loader` function's `max_length` parameter is critical for adjusting sequence lengths

### [`VTNet_User_Aggregate`](./VTNet_across_tasks/)
VTNet implementations for the evaluating per user VTNet dataset:
- ğŸ”„ Cross-task prediction configurations
- ğŸ“„ Documentation on required model modifications
- ğŸ“ The extra preprocessing done to make the within-tasks dataset to per user

## ğŸ› ï¸ Recommended Directory Structure for VTNet (HPC)

While multiple directory structures are possible, here is our recommended setup:

```
VTNet/
â”œâ”€â”€ dataset/                 # Dataset pre-processed via our functions in `preprocessing`        
â”œâ”€â”€ utils.py                 # Utility functions used in VTNet
â”œâ”€â”€ vislit.pickle            # Pickle file for grouped CV in classifying visual literacy
â”œâ”€â”€ readp.pickle             # Pickle file for grouped CV in classifying reading proficiency
â”œâ”€â”€ verbalwm.pickle          # Pickle file for grouped CV in classifying verbal working memory 
â””â”€â”€ vtenv/                   # vtenv environments, one per new length cut-off (ex. vtenv14)
    â”œâ”€â”€ VisLit/
    â”‚   â”œâ”€â”€ VTNet.py         # VTNet modified to be trained on VisLit 
    â”‚   â””â”€â”€ run.sh           # Bash script to execute VTNet for Visual Literacies tasks
    â”œâ”€â”€ ReadP/
    â”‚   â”œâ”€â”€ VTNet.py         # VTNet modified to be trained on ReadP
    â”‚   â””â”€â”€ run.sh           # Bash script to execute VTNet for Reading Proficiency tasks
    â””â”€â”€ VerbalWM/
        â”œâ”€â”€ VTNet.py         # VTNet modified to be trained on VerbalWM
        â””â”€â”€ run.sh           # Bash script to execute VTNet for Verbal Working Memory tasks
```

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8 or higher
- Conda (for environment setup)

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/ethanwongca/hai_work
   ```

2. **Navigate to the project directory**
   ```bash
   cd hai_work
   ```

3. **Create and activate the Conda environment**
   ```bash
   conda env create -f environment.yml
   conda activate hai-env
   ```

## ğŸ“š Key Concepts

### VTNet Architecture
VTNet is a neural architecture that processes eye-tracking data to predict user confusion and Alzheimer disease. It transforms sequential eye movement data into features useful for classification tasks.

### Sequence Length Configuration
When using VTNet, pay attention to the `max_length` parameter in the `st_pickle_loader` function. This parameter adjusts the model for different sequence lengths (e.g., from 1000 to 3000).

## ğŸ“– References

For a deeper understanding of the methods used in this repository, we recommend these papers:

- Barral, O., LallÃ©, S., Guz, G., Iranpour, A., & Conati, C. (2020). [Eyeâ€‘tracking to predict user cognitive abilities and performance for userâ€‘adaptive narrative visualizations](https://doi.org/10.1145/3382507.3418884). *In Proceedings of the 2020 International Conference on Multimodal Interaction* (pp. 163â€“173).

- Sims, S. D., & Conati, C. (2020). [A neural architecture for detecting user confusion in eyeâ€‘tracking data](https://doi.org/10.1145/3382507.3418828). *In Proceedings of the 2020 International Conference on Multimodal Interaction* (pp. 15â€“23).

- Sriram, H., Conati, C., & Field, T. (2023). [Classification of Alzheimer's disease with deep learning on eyeâ€‘tracking data](https://doi.org/10.1145/3577190.3614149). *In Proceedings of the 25th International Conference on Multimodal Interaction* (pp. 104â€“113).


## ğŸ¤ Acknowledgments

- UBC's Human Artificial Intelligence Interaction Lab

